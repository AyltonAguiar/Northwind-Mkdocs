{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"In\u00edcio"},{"location":"src/documentacao/arquitetura/","text":"Objetivo \u00b6 O objetivo do documento \u00e9 exibir a arquitetura utilizada no desenvolvimento da documenta\u00e7\u00e3o utilizando o gerador de p\u00e1ginas est\u00e1tico MKDocs . Organiza\u00e7\u00e3o \u00b6 Existe duas propostas de padr\u00f5es utilizados nesse projeto: Ambiente Descri\u00e7\u00e3o Desenvolvimento Padr\u00e3o para documentar o desenvolvimento da aplica\u00e7\u00e3o, onde h\u00e1 a descri\u00e7\u00e3o das funcionalidades e implementa\u00e7\u00f5es de comandos e c\u00f3digos Arquitetura Descrever a arquitetura ou a funcionalidade de forma mais conceitual, sem muitos detalhes de implementa\u00e7\u00e3o Arquitetura do Projeto \u00b6 A documenta\u00e7\u00e3o \u00e9 composta de v\u00e1rios arquivos, aqui darei foco em docs\\assets e docs\\src . docs\\assets : Cont\u00e9m arquivos que n\u00e3o s\u00e3o .md . Focado em imagens e outros arquivos secund\u00e1rios para anexar \u00e0 documenta\u00e7\u00e3o. docs\\src : Concentra a documenta\u00e7\u00e3o em markdown. Com isso em mente, teremos a seguinte estrutura: \ud83d\udce6REPOSIT\u00d3RIO \u2523 \ud83d\udcc2docs \u2503 \u2523 \ud83d\udcc2assets \u2503 \u2503 \u2523 \ud83d\udcc2projeto1 \u2503 \u2503 \u2503 \u2517 \ud83d\udcc2bagdes \u2503 \u2503 \u2503 \u2503 \u2517 \ud83d\udcf7image.png \u2503 \u2523 \ud83d\udcc2src \u2503 \u2503 \u2523 \ud83d\udcc2projeto1 \u2503 \u2503 \u2503 \u2523 \ud83d\udcc2desenvolvimento \u2503 \u2503 \u2503 \u2503 \u2517 \ud83d\udcdcindex.md \u2503 \u2503 \u2503 \u2523 \ud83d\udcc2arquitetura \u2503 \u2503 \u2503 \u2503 \u2517 \ud83d\udcdcarquitetura.md \u2523 \ud83d\udcdcmkdocs.yml # (1) \u2517 \ud83d\udcdcrequirements.txt Arquivo de configura\u00e7\u00e3o do Mkdocks: thema, navega\u00e7\u00e3o, plugins, reposit\u00f3rio, etc.","title":"Arquitetura"},{"location":"src/documentacao/arquitetura/#objetivo","text":"O objetivo do documento \u00e9 exibir a arquitetura utilizada no desenvolvimento da documenta\u00e7\u00e3o utilizando o gerador de p\u00e1ginas est\u00e1tico MKDocs .","title":"Objetivo"},{"location":"src/documentacao/arquitetura/#organizacao","text":"Existe duas propostas de padr\u00f5es utilizados nesse projeto: Ambiente Descri\u00e7\u00e3o Desenvolvimento Padr\u00e3o para documentar o desenvolvimento da aplica\u00e7\u00e3o, onde h\u00e1 a descri\u00e7\u00e3o das funcionalidades e implementa\u00e7\u00f5es de comandos e c\u00f3digos Arquitetura Descrever a arquitetura ou a funcionalidade de forma mais conceitual, sem muitos detalhes de implementa\u00e7\u00e3o","title":"Organiza\u00e7\u00e3o"},{"location":"src/documentacao/arquitetura/#arquitetura-do-projeto","text":"A documenta\u00e7\u00e3o \u00e9 composta de v\u00e1rios arquivos, aqui darei foco em docs\\assets e docs\\src . docs\\assets : Cont\u00e9m arquivos que n\u00e3o s\u00e3o .md . Focado em imagens e outros arquivos secund\u00e1rios para anexar \u00e0 documenta\u00e7\u00e3o. docs\\src : Concentra a documenta\u00e7\u00e3o em markdown. Com isso em mente, teremos a seguinte estrutura: \ud83d\udce6REPOSIT\u00d3RIO \u2523 \ud83d\udcc2docs \u2503 \u2523 \ud83d\udcc2assets \u2503 \u2503 \u2523 \ud83d\udcc2projeto1 \u2503 \u2503 \u2503 \u2517 \ud83d\udcc2bagdes \u2503 \u2503 \u2503 \u2503 \u2517 \ud83d\udcf7image.png \u2503 \u2523 \ud83d\udcc2src \u2503 \u2503 \u2523 \ud83d\udcc2projeto1 \u2503 \u2503 \u2503 \u2523 \ud83d\udcc2desenvolvimento \u2503 \u2503 \u2503 \u2503 \u2517 \ud83d\udcdcindex.md \u2503 \u2503 \u2503 \u2523 \ud83d\udcc2arquitetura \u2503 \u2503 \u2503 \u2503 \u2517 \ud83d\udcdcarquitetura.md \u2523 \ud83d\udcdcmkdocs.yml # (1) \u2517 \ud83d\udcdcrequirements.txt Arquivo de configura\u00e7\u00e3o do Mkdocks: thema, navega\u00e7\u00e3o, plugins, reposit\u00f3rio, etc.","title":"Arquitetura do Projeto"},{"location":"src/documentacao/iniciando/","text":"Apresenta\u00e7\u00e3o \u00b6 O intuito \u00e9 auxiliar na reprodu\u00e7\u00e3o da documenta\u00e7\u00e3o realizada no projeto. Pr\u00e9-requisitos \u00b6 A m\u00e1quina deve conter python (prefer\u00eancia acima da vers\u00e3o 3.8) e pip . N\u00e3o tenho o pip, como baixar? Atalho para o site: Pypi-pip Como fa\u00e7o para instalar o Python? Aqui nesse t\u00f3pico passo alguns links: tutorial Come\u00e7ando \u00b6 Para iniciar a documenta\u00e7\u00e3o apresentada, pode-se utilizar uma venv e o arquivo requirements.txt . Assim voc\u00ea possui um projeto reservado utilizando dos pr\u00f3s do ambiente virtual. criar ambiente virtual python - m venv . venv ativar ambiente virtual . \\ . venv \\ Scripts \\ Activate Instala\u00e7\u00e3o dos requirements pip install - r requirements . txt Como usar o ambiente virtual? criar: python - m venv . venv ativar: . \\ . venv \\ Scripts \\ Activate desativar: deactivate Iniciando Mkdocs \u00b6 Ap\u00f3s a execu\u00e7\u00e3o do comando mkdocs serve , o servidor ser\u00e1 iniciado e poder\u00e1 ser acessado em http://127.0.0.1:8000 . Iniciando servidor local mkdocs serve","title":"Iniciando"},{"location":"src/documentacao/iniciando/#apresentacao","text":"O intuito \u00e9 auxiliar na reprodu\u00e7\u00e3o da documenta\u00e7\u00e3o realizada no projeto.","title":"Apresenta\u00e7\u00e3o"},{"location":"src/documentacao/iniciando/#pre-requisitos","text":"A m\u00e1quina deve conter python (prefer\u00eancia acima da vers\u00e3o 3.8) e pip . N\u00e3o tenho o pip, como baixar? Atalho para o site: Pypi-pip Como fa\u00e7o para instalar o Python? Aqui nesse t\u00f3pico passo alguns links: tutorial","title":"Pr\u00e9-requisitos"},{"location":"src/documentacao/iniciando/#comecando","text":"Para iniciar a documenta\u00e7\u00e3o apresentada, pode-se utilizar uma venv e o arquivo requirements.txt . Assim voc\u00ea possui um projeto reservado utilizando dos pr\u00f3s do ambiente virtual. criar ambiente virtual python - m venv . venv ativar ambiente virtual . \\ . venv \\ Scripts \\ Activate Instala\u00e7\u00e3o dos requirements pip install - r requirements . txt Como usar o ambiente virtual? criar: python - m venv . venv ativar: . \\ . venv \\ Scripts \\ Activate desativar: deactivate","title":"Come\u00e7ando"},{"location":"src/documentacao/iniciando/#iniciando-mkdocs","text":"Ap\u00f3s a execu\u00e7\u00e3o do comando mkdocs serve , o servidor ser\u00e1 iniciado e poder\u00e1 ser acessado em http://127.0.0.1:8000 . Iniciando servidor local mkdocs serve","title":"Iniciando Mkdocs"},{"location":"src/documentacao/publicacao-mkdocs/","text":"Publicando Mkdocs \u00b6 Mkdocs github manual \u00b6 No diret\u00f3rio da documenta\u00e7\u00e3o utilize o comando mkdocs build para gerar o diret\u00f3rio \\site com a estrutura. Ap\u00f3s gerar o diret\u00f3rio \\site utilize o comando mkdocs gh-deploy --force . mkdocs build mkdocs gh-deploy --force Mkdocs github autom\u00e1tico \u00b6 O Modelo autom\u00e1tico \u00e9 realizado pela cria\u00e7\u00e3o de um workflow no github. O CI \u00e9 configurado no arquivo .github/workflows/ci.yml mkdocs-ci.yml name: ci on: push: branches: - master - main permissions: contents: write jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: 3.x - uses: actions/cache@v2 with: key: ${{ github.ref }} path: .cache - run: pip install -r requirements.txt - run: pip install \\ mkdocs-material \\ mkdocs-table-reader-plugin - run: mkdocs gh-deploy --force Aten\u00e7\u00e3o Se a p\u00e1gina do GitHub n\u00e3o aparecer ap\u00f3s alguns minutos, v\u00e1 para as configura\u00e7\u00f5es do seu reposit\u00f3rio e certifique-se de que a ramifica\u00e7\u00e3o da fonte de publica\u00e7\u00e3o da sua p\u00e1gina do GitHub esteja definida como gh-pages. Aguarde uns alguns instantes para verificar a publica\u00e7\u00e3o do site est\u00e1tico.","title":"Publicando Mkdocs"},{"location":"src/documentacao/publicacao-mkdocs/#publicando-mkdocs","text":"","title":"Publicando Mkdocs"},{"location":"src/documentacao/publicacao-mkdocs/#mkdocs-github-manual","text":"No diret\u00f3rio da documenta\u00e7\u00e3o utilize o comando mkdocs build para gerar o diret\u00f3rio \\site com a estrutura. Ap\u00f3s gerar o diret\u00f3rio \\site utilize o comando mkdocs gh-deploy --force . mkdocs build mkdocs gh-deploy --force","title":"Mkdocs github manual"},{"location":"src/documentacao/publicacao-mkdocs/#mkdocs-github-automatico","text":"O Modelo autom\u00e1tico \u00e9 realizado pela cria\u00e7\u00e3o de um workflow no github. O CI \u00e9 configurado no arquivo .github/workflows/ci.yml mkdocs-ci.yml name: ci on: push: branches: - master - main permissions: contents: write jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: 3.x - uses: actions/cache@v2 with: key: ${{ github.ref }} path: .cache - run: pip install -r requirements.txt - run: pip install \\ mkdocs-material \\ mkdocs-table-reader-plugin - run: mkdocs gh-deploy --force Aten\u00e7\u00e3o Se a p\u00e1gina do GitHub n\u00e3o aparecer ap\u00f3s alguns minutos, v\u00e1 para as configura\u00e7\u00f5es do seu reposit\u00f3rio e certifique-se de que a ramifica\u00e7\u00e3o da fonte de publica\u00e7\u00e3o da sua p\u00e1gina do GitHub esteja definida como gh-pages. Aguarde uns alguns instantes para verificar a publica\u00e7\u00e3o do site est\u00e1tico.","title":"Mkdocs github autom\u00e1tico"},{"location":"src/documentacao/referencias/","text":"Refer\u00eancias \u00b6 Para a produ\u00e7\u00e3o do conte\u00fado foram utilizadas as seguintes refer\u00eancias: Mkdocs Material, Github Github Pages Github Actions Github Docs, github pages","title":"Refer\u00eancias"},{"location":"src/documentacao/referencias/#referencias","text":"Para a produ\u00e7\u00e3o do conte\u00fado foram utilizadas as seguintes refer\u00eancias: Mkdocs Material, Github Github Pages Github Actions Github Docs, github pages","title":"Refer\u00eancias"},{"location":"src/engdados/","text":"Engdados \u00e9 um projeto de dados com intuito de mostrar como podemos utilizar as tecnologias: Python, Terraform, Redshift, Data build tool Cloud (dbt cloud) e Looker Studio. O projeto \u00e9 baseado no curso Forma\u00e7\u00e3o Engenharia de Dados: Domine Big Data! do instrutor Fernando Amaral .","title":"Sobre"},{"location":"src/engdados/introducao/","text":"Introdu\u00e7\u00e3o \u00b6 O projeto Northwind \u00e9 um DataWarehouse que possui solu\u00e7\u00e3o de armazenamento de dados utilizada para consolidar informa\u00e7\u00f5es em um \u00fanico local, tornando-as dispon\u00edveis para an\u00e1lise e gera\u00e7\u00e3o de relat\u00f3rios. Projetado para suportar o processo de tomada de decis\u00e3o da empresa, ajudando a identificar tend\u00eancias e oportunidades de neg\u00f3cio.","title":"Introdu\u00e7\u00e3o"},{"location":"src/engdados/introducao/#introducao","text":"O projeto Northwind \u00e9 um DataWarehouse que possui solu\u00e7\u00e3o de armazenamento de dados utilizada para consolidar informa\u00e7\u00f5es em um \u00fanico local, tornando-as dispon\u00edveis para an\u00e1lise e gera\u00e7\u00e3o de relat\u00f3rios. Projetado para suportar o processo de tomada de decis\u00e3o da empresa, ajudando a identificar tend\u00eancias e oportunidades de neg\u00f3cio.","title":"Introdu\u00e7\u00e3o"},{"location":"src/engdados/prerequisitos/","text":"Objetivo \u00b6 Necess\u00e1rios para prosseguir na constru\u00e7\u00e3o do projeto. Cada t\u00f3pico dos pr\u00e9-requisitos possuem links com os tutoriais de instala\u00e7\u00f5es para auxiliar caso n\u00e3o possua. Contas \u00b6 S\u00e3o os principais para conseguir avan\u00e7ar no projeto, seja para provisionamento na AWS ou na cria\u00e7\u00e3o das visualiza\u00e7\u00f5es, tabelas no DBT. Atalhos Descri\u00e7\u00e3o Criar conta gratuita da AWS Site da AWS Criar conta gratuita do Dbt Site do Dbt CLI Terraform \u00b6 Abaixo o tutorial de instala\u00e7\u00e3o disponibilizado pela Hashicorp. Necess\u00e1rio para que possamos trabalhar com terraform na nossa m\u00e1quina. Atalhos Descri\u00e7\u00e3o Tutorial de instala\u00e7\u00e3o CLI Terraform CLI Terraform CLI Aws \u00b6 N\u00e3o faz parte das boas pr\u00e1ticas a gera\u00e7\u00e3o de uma chave de acesso na conta raiz . Entendido isso, vamos aos links abaixo: Atalhos Descri\u00e7\u00e3o Tutorial de instala\u00e7\u00e3o CLI AWS CLI AWS Criar e excluir chave de acesso AWS Key Access Observa\u00e7\u00e3o: Ser\u00e1 utilizado o profile da cli aws, e o motivo desse \u00e9 para evitar qualquer tipo de compartilhamento das suas chaves de acesso. Atalhos Descri\u00e7\u00e3o Configurando profile da cli aws AWS CLI Command Reference Python \u00b6 Abaixo guia oficial de instala\u00e7\u00e3o e link de download Atalhos Descri\u00e7\u00e3o Tutorial de instala\u00e7\u00e3o Python Site Python.org P\u00e1gina de Download Python Site Python.org, Lista para download Abaixo Tutoriais de instala\u00e7\u00e3o: Streaming Descri\u00e7\u00e3o Autor OS Tutorial Python + Visual Studio Code Ot\u00e1vio Miranda Windows","title":"Pr\u00e9-Requisitos"},{"location":"src/engdados/prerequisitos/#objetivo","text":"Necess\u00e1rios para prosseguir na constru\u00e7\u00e3o do projeto. Cada t\u00f3pico dos pr\u00e9-requisitos possuem links com os tutoriais de instala\u00e7\u00f5es para auxiliar caso n\u00e3o possua.","title":"Objetivo"},{"location":"src/engdados/prerequisitos/#contas","text":"S\u00e3o os principais para conseguir avan\u00e7ar no projeto, seja para provisionamento na AWS ou na cria\u00e7\u00e3o das visualiza\u00e7\u00f5es, tabelas no DBT. Atalhos Descri\u00e7\u00e3o Criar conta gratuita da AWS Site da AWS Criar conta gratuita do Dbt Site do Dbt","title":"Contas"},{"location":"src/engdados/prerequisitos/#cli-terraform","text":"Abaixo o tutorial de instala\u00e7\u00e3o disponibilizado pela Hashicorp. Necess\u00e1rio para que possamos trabalhar com terraform na nossa m\u00e1quina. Atalhos Descri\u00e7\u00e3o Tutorial de instala\u00e7\u00e3o CLI Terraform CLI Terraform","title":"CLI Terraform"},{"location":"src/engdados/prerequisitos/#requisito-aws-cli","text":"N\u00e3o faz parte das boas pr\u00e1ticas a gera\u00e7\u00e3o de uma chave de acesso na conta raiz . Entendido isso, vamos aos links abaixo: Atalhos Descri\u00e7\u00e3o Tutorial de instala\u00e7\u00e3o CLI AWS CLI AWS Criar e excluir chave de acesso AWS Key Access Observa\u00e7\u00e3o: Ser\u00e1 utilizado o profile da cli aws, e o motivo desse \u00e9 para evitar qualquer tipo de compartilhamento das suas chaves de acesso. Atalhos Descri\u00e7\u00e3o Configurando profile da cli aws AWS CLI Command Reference","title":"CLI Aws"},{"location":"src/engdados/prerequisitos/#tutorial-python","text":"Abaixo guia oficial de instala\u00e7\u00e3o e link de download Atalhos Descri\u00e7\u00e3o Tutorial de instala\u00e7\u00e3o Python Site Python.org P\u00e1gina de Download Python Site Python.org, Lista para download Abaixo Tutoriais de instala\u00e7\u00e3o: Streaming Descri\u00e7\u00e3o Autor OS Tutorial Python + Visual Studio Code Ot\u00e1vio Miranda Windows","title":"Python"},{"location":"src/engdados/arquitetura/arquitetura/","text":"Introdu\u00e7\u00e3o \u00b6 A arquitetura do Data Warehouse utiliza a plataforma Redshift da Amazon como banco de dados, com armazenamento em S3 e gerenciamento de segredos com Secrets Manager . O gerenciamento de recursos e adi\u00e7\u00e3o de objetos s\u00e3o feitos com Terraform e a transforma\u00e7\u00e3o de dados \u00e9 realizada com DBT Cloud . A visualiza\u00e7\u00e3o \u00e9 feita com Looker Studio . A automa\u00e7\u00e3o da transi\u00e7\u00e3o de dados \u00e9 realizada pelo Python . Tecnologias \u00b6 Linguagens, ferramentas, etc Descri\u00e7\u00e3o Python Linguagem de programa\u00e7\u00e3o de alto n\u00edvel AWS Plataforma de servi\u00e7os de computa\u00e7\u00e3o em nuvem, que formam uma plataforma de computa\u00e7\u00e3o na nuvem oferecida pela Amazon.com Terraform Ferramenta de software de c\u00f3digo aberto, infraestrutura como configura\u00e7\u00e3o, criada pela HashiCorp dbt Cloud O dbt Cloud\u2122 \u00e9 a maneira mais r\u00e1pida e confi\u00e1vel de implantar o dbt. Desenvolva, teste, programe e investigue modelos de dados, tudo em uma interface do usu\u00e1rio baseada na web. Looker Studio Ferramenta gratuita que transforma seus dados em relat\u00f3rios e pain\u00e9is informativos totalmente personaliz\u00e1veis, f\u00e1ceis de ler e de compartilhar Diagrama \u00b6 Banco de Dados \u00b6 O banco de dados central utilizado na arquitetura do projeto \u00e9 o Amazon Redshift , que \u00e9 escal\u00e1vel e oferece alta disponibilidade. Expanda para visualizar exemplo Integra\u00e7\u00e3o de Dados \u00b6 No momento os dados s\u00e3o integrados utilizando terraform durante o provisionamento dos recursos, que s\u00e3o executados de forma manual. Os dados integrados s\u00e3o armazenados no Amazon S3 . Expanda para visualizar exemplo Armazenamento de Dados \u00b6 Os dados integrados s\u00e3o armazenados no Amazon Redshift com aux\u00edlio de scripts em Python que realizam a leitura, tratamento e armazenamento. Posteriormente s\u00e3o transformados no DBT cloud e armazenados em schemas espec\u00edficos para visualisa\u00e7\u00e3o e an\u00e1lise dos dados. Expanda para visualizar exemplo Transforma\u00e7\u00e3o de Dados \u00b6 A transforma\u00e7\u00e3o dos dados \u00e9 realizada com o DBT Cloud , que permite a cria\u00e7\u00e3o de scripts para a transforma\u00e7\u00e3o dos dados armazenados no Amazon Redshift . Al\u00e9m de cria\u00e7\u00e3o de Jobs e documenta\u00e7\u00f5es. Expanda para visualizar exemplo An\u00e1lise de Dados \u00b6 A an\u00e1lise de dados \u00e9 realizada atrav\u00e9s do Looker Studio , que permite a cria\u00e7\u00e3o de relat\u00f3rios e visualiza\u00e7\u00f5es interativas dos dados armazenados. Seguran\u00e7a \u00b6 A seguran\u00e7a dos dados \u00e9 garantida com o uso do Amazon Secrets Manager para armazenamento seguro das credenciais de acesso, al\u00e9m da seguran\u00e7a oferecida pelo Amazon Redshift e Amazon S3 . Expanda para visualizar exemplo","title":"Arquitetura"},{"location":"src/engdados/arquitetura/arquitetura/#introducao","text":"A arquitetura do Data Warehouse utiliza a plataforma Redshift da Amazon como banco de dados, com armazenamento em S3 e gerenciamento de segredos com Secrets Manager . O gerenciamento de recursos e adi\u00e7\u00e3o de objetos s\u00e3o feitos com Terraform e a transforma\u00e7\u00e3o de dados \u00e9 realizada com DBT Cloud . A visualiza\u00e7\u00e3o \u00e9 feita com Looker Studio . A automa\u00e7\u00e3o da transi\u00e7\u00e3o de dados \u00e9 realizada pelo Python .","title":"Introdu\u00e7\u00e3o"},{"location":"src/engdados/arquitetura/arquitetura/#tecnologias","text":"Linguagens, ferramentas, etc Descri\u00e7\u00e3o Python Linguagem de programa\u00e7\u00e3o de alto n\u00edvel AWS Plataforma de servi\u00e7os de computa\u00e7\u00e3o em nuvem, que formam uma plataforma de computa\u00e7\u00e3o na nuvem oferecida pela Amazon.com Terraform Ferramenta de software de c\u00f3digo aberto, infraestrutura como configura\u00e7\u00e3o, criada pela HashiCorp dbt Cloud O dbt Cloud\u2122 \u00e9 a maneira mais r\u00e1pida e confi\u00e1vel de implantar o dbt. Desenvolva, teste, programe e investigue modelos de dados, tudo em uma interface do usu\u00e1rio baseada na web. Looker Studio Ferramenta gratuita que transforma seus dados em relat\u00f3rios e pain\u00e9is informativos totalmente personaliz\u00e1veis, f\u00e1ceis de ler e de compartilhar","title":"Tecnologias"},{"location":"src/engdados/arquitetura/arquitetura/#diagrama","text":"","title":"Diagrama"},{"location":"src/engdados/arquitetura/arquitetura/#banco-de-dados","text":"O banco de dados central utilizado na arquitetura do projeto \u00e9 o Amazon Redshift , que \u00e9 escal\u00e1vel e oferece alta disponibilidade. Expanda para visualizar exemplo","title":"Banco de Dados"},{"location":"src/engdados/arquitetura/arquitetura/#integracao-de-dados","text":"No momento os dados s\u00e3o integrados utilizando terraform durante o provisionamento dos recursos, que s\u00e3o executados de forma manual. Os dados integrados s\u00e3o armazenados no Amazon S3 . Expanda para visualizar exemplo","title":"Integra\u00e7\u00e3o de Dados"},{"location":"src/engdados/arquitetura/arquitetura/#armazenamento-de-dados","text":"Os dados integrados s\u00e3o armazenados no Amazon Redshift com aux\u00edlio de scripts em Python que realizam a leitura, tratamento e armazenamento. Posteriormente s\u00e3o transformados no DBT cloud e armazenados em schemas espec\u00edficos para visualisa\u00e7\u00e3o e an\u00e1lise dos dados. Expanda para visualizar exemplo","title":"Armazenamento de Dados"},{"location":"src/engdados/arquitetura/arquitetura/#transformacao-de-dados","text":"A transforma\u00e7\u00e3o dos dados \u00e9 realizada com o DBT Cloud , que permite a cria\u00e7\u00e3o de scripts para a transforma\u00e7\u00e3o dos dados armazenados no Amazon Redshift . Al\u00e9m de cria\u00e7\u00e3o de Jobs e documenta\u00e7\u00f5es. Expanda para visualizar exemplo","title":"Transforma\u00e7\u00e3o de Dados"},{"location":"src/engdados/arquitetura/arquitetura/#analise-de-dados","text":"A an\u00e1lise de dados \u00e9 realizada atrav\u00e9s do Looker Studio , que permite a cria\u00e7\u00e3o de relat\u00f3rios e visualiza\u00e7\u00f5es interativas dos dados armazenados.","title":"An\u00e1lise de Dados"},{"location":"src/engdados/arquitetura/arquitetura/#seguranca","text":"A seguran\u00e7a dos dados \u00e9 garantida com o uso do Amazon Secrets Manager para armazenamento seguro das credenciais de acesso, al\u00e9m da seguran\u00e7a oferecida pelo Amazon Redshift e Amazon S3 . Expanda para visualizar exemplo","title":"Seguran\u00e7a"},{"location":"src/engdados/desenvolvimento/","text":"Desenvolvimento \u00b6 Objetivo \u00b6 Descrever partes do processo de desenvolvimento do projeto. Seja para entender como foi constru\u00eddo ou para tentar replicar o projeto. Divis\u00e3o \u00b6 Usando Terraform para provisionar infraestrutura Usando Python para automatizar Utiliza\u00e7\u00e3o do dbt para o SQL Utiliza\u00e7\u00e3o do Looker Studio para visuali\u00e7\u00e3o dos dados","title":"Desenvolvimento"},{"location":"src/engdados/desenvolvimento/#desenvolvimento","text":"","title":"Desenvolvimento"},{"location":"src/engdados/desenvolvimento/#objetivo","text":"Descrever partes do processo de desenvolvimento do projeto. Seja para entender como foi constru\u00eddo ou para tentar replicar o projeto.","title":"Objetivo"},{"location":"src/engdados/desenvolvimento/#divisao","text":"Usando Terraform para provisionar infraestrutura Usando Python para automatizar Utiliza\u00e7\u00e3o do dbt para o SQL Utiliza\u00e7\u00e3o do Looker Studio para visuali\u00e7\u00e3o dos dados","title":"Divis\u00e3o"},{"location":"src/engdados/desenvolvimento/aws_terraform/","text":"Terraform \u00b6 Provisionando remote state no S3 \u00b6 No Script requisitamos o ID da conta AWS utilizando o profile e region para poder criar o bucket com Id \u00fanico. Adicionamos tags, versionamento e acl private ao provisionamento do bucket S3. main.tf terraform\\01_terraform_remote_state\\main.tf Recursos provisionados \u00b6 Recursos do Provedor Descri\u00e7\u00e3o aws_s3_bucket Fornece um recurso de bucket do S3 aws_s3_bucket_acl Configura\u00e7\u00e3o de ACL no bucket aws_s3_bucket_versioning Configura\u00e7\u00e3o de versionamento no bucket Ordem da execu\u00e7\u00e3o dos comandos \u00b6 Abaixo clique em expandir para visualizar a ordem da execu\u00e7\u00e3o dos comandos caso necessite. A mesma ordem ser\u00e1 utilizada nos demais t\u00f3picos. Inicializando a configura\u00e7\u00e3o do Terraform terraform init Validando sua configura\u00e7\u00e3o terraform validate Executando um plano de cria\u00e7\u00e3o terraform plan Ap\u00f3s revisar o plano de cria\u00e7\u00e3o, vamos aplicar e aprovar a execu\u00e7\u00e3o automaticamente terraform apply -auto-approve Provisionando bucket S3 com arquivos \u00b6 Provisionamento do bucket S3 com adi\u00e7\u00e3o de arquivos em csv e um jpg. \u00c9 utlizado backend e module no script main. O S3 cont\u00e9m ciclo de vida, versionamento, ACL e adi\u00e7\u00e3o de objetos. main.tf terraform\\02_aws_s3_and_files\\main.tf Recursos provisionados \u00b6 Recursos do Provedor Descri\u00e7\u00e3o aws_s3_bucket Esta funcionalidade \u00e9 para gerenciar o S3 em uma parti\u00e7\u00e3o AWS aws_s3_bucket_acl Fornece um recurso de ACL de bucket do S3 aws_s3_bucket_lifecycle_configuration Fornece um recurso de configura\u00e7\u00e3o independente para a configura\u00e7\u00e3o do ciclo de vida do bucket do S3 aws_s3_bucket_metric Fornece um recurso de configura\u00e7\u00e3o de m\u00e9tricas de bucket do S3 aws_s3_bucket_versioning Fornece um recurso para controlar o controle de vers\u00e3o em um bucket do S3 aws_s3_object Fornece um recurso de objeto S3 Recursos Terraform Descri\u00e7\u00e3o random_pet Gera nomes de animais aleat\u00f3rios que podem ser usados como identificadores exclusivos para outros recursos Ordem da execu\u00e7\u00e3o dos comandos \u00b6 Clique aqui para ir na ordem dos comandos Provisionando Redshift e outros recursos necess\u00e1rios. \u00b6 O script possui a cria\u00e7\u00e3o do cluster Redshift, cria\u00e7\u00e3o de usu\u00e1rios e grupos para Redshift, adi\u00e7\u00e3o de role e policy, cria\u00e7\u00e3o de secrets manager com adi\u00e7\u00e3o de informa\u00e7\u00f5es criadas no processo para n\u00e3o utilizar as senhas diretamente no c\u00f3digo, adi\u00e7\u00e3o de IP'S para acesso aos recursos provisionados. Pontuarei apenas algumas partes que precisam de modifica\u00e7\u00f5es: main.tf terraform\\03_terraform_remote_state\\main.tf Cria\u00e7\u00e3o Cluster Redshift \u00b6 Esse cluster atende as caracteristicas do modelo free tier, caso esteja dentro do prazo. Pode-se modificar master_username e database_name . resource \"aws_redshift_cluster\" \"cluster_reds\" { clus ter _ide nt i f ier = ra n dom_pe t . t his.id da ta base_ na me = \"northwind\" mas ter _user na me = \"aylton\" mas ter _password = ra n dom_password.password [ 0 ] .resul t por t = 5439 n ode_ t ype = \"dc2.large\" clus ter _ t ype = \"single-node\" publicly_accessible = true apply_immedia tel y = true # Necess\u00e1rio para des tr oy do clus ter qua n do t iver s na psho t skip_ f i nal _s na psho t = true # Necess\u00e1rio para des tr oy do clus ter qua n do t iver s na psho t ta gs = var. ta gs } Adi\u00e7\u00e3o de entrada CIDR \u00b6 Foram criadas 3 adi\u00e7\u00f5es de CIDR: Meu IP, Entrada para o dbt e Entrada do Looker Studio. E Fa\u00e7a altera\u00e7\u00e3o para seu IP, sem ele voc\u00ea n\u00e3o conseguir\u00e1 realizar opera\u00e7\u00f5es futuras. Pode-se modificar description e cidr_blocks . \"security_rule\" \"security_rule_dbt\" \"security_rule_lookerstudio\" resource \"aws_security_group_rule\" \"security_rule\" { descrip t io n = \"Redshift Aylton\" t ype = \"ingress\" fr om_por t = 5439 t o_por t = 5439 pro t ocol = \"tcp\" cidr_blocks = [ \"${chomp(data.http.meu_ip.response_body)}/32\" ] # meu IP securi t y_group_id = da ta .aws_securi t y_group.securi t y_group_da ta .id } resource \"aws_security_group_rule\" \"security_rule_dbt\" { descrip t io n = \"DBT 52.45\" t ype = \"ingress\" fr om_por t = 5439 t o_por t = 5439 pro t ocol = \"tcp\" cidr_blocks = [ \"52.45.144.63/32\" , \"54.81.134.249/32\" , \"52.22.161.231/32\" ] # IP db t securi t y_group_id = da ta .aws_securi t y_group.securi t y_group_da ta .id } resource \"aws_security_group_rule\" \"security_rule_lookerstudio\" { descrip t io n = \"looker_studio\" t ype = \"ingress\" fr om_por t = 5439 t o_por t = 5439 pro t ocol = \"tcp\" cidr_blocks = [ \"142.251.74.0/23\" , \"74.125.0.0/16\" ] # IP Looker s tu dio securi t y_group_id = da ta .aws_securi t y_group.securi t y_group_da ta .id } Adicionado dado de captura autom\u00e1tica ip da ta \"http\" \"meu_ip\" { url = \"https://ipv4.icanhazip.com\" } Agora toda vez que seu Ip mudar, voc\u00ea n\u00e3o precisa mais alterar manualmente o n\u00famero do IP. Basta usar o terraform plan para verificar as mudan\u00e7as e terraform apply -auto-approve para aplic\u00e1-las. Cria\u00e7\u00e3o de usu\u00e1rios, grupos e adi\u00e7\u00e3o de usu\u00e1rio ao grupo \u00b6 Recurso Removido Os recursos espec\u00edficos para este t\u00f3pico estavam gerando instabilidades no terraform, ent\u00e3o a cria\u00e7\u00e3o dos usu\u00e1rios, grupos e adi\u00e7\u00e3o de usu\u00e1rios ao grupos foram movidos para o python no t\u00f3pico Cria\u00e7\u00e3o de usu\u00e1rios, grupos e vinculos Salvando usu\u00e1rios e senhas no Secret Manager \u00b6 Um recurso muito importante quando trata-se de guardar dados mais sens\u00edveis, principalmente quando estamos gerando senhas e logins como \u00e9 caso desse pequeno projeto. O script vai guardar os logins e senhas referente ao Redshift, al\u00e9m de outras informa\u00e7\u00f5es para serem utilizadas no python futuramente. resource \"aws_secretsmanager_secret_version\" \"rd_secret_version\" { secre t _id = aws_secre ts ma na ger_secre t .rd_secre t .id secre t _s tr i n g = jso nen code( { e n gi ne = \"redshift\" da ta _base = aws_redshi ft _clus ter .clus ter _reds.da ta base_ na me user na me = aws_redshi ft _clus ter .clus ter _reds.mas ter _user na me password = aws_redshi ft _clus ter .clus ter _reds.mas ter _password por t = aws_redshi ft _clus ter .clus ter _reds.por t hos t = aws_redshi ft _clus ter .clus ter _reds.e n dpoi nt dbClus ter Ide nt i f ier = aws_redshi ft _clus ter .clus ter _reds.clus ter _ide nt i f ier db t _prod = \"dbt_prod\" db t _prod_password = ra n dom_password.password [ 1 ] .resul t db t _dev = \"dbt_dev\" db t _dev_password = ra n dom_password.password [ 2 ] .resul t looker_user = \"looker\" looker_password = ra n dom_password.password [ 3 ] .resul t } ) } Recursos provisionados \u00b6 Recursos do Provedor Descri\u00e7\u00e3o aws_iam_policy Fornece uma pol\u00edtica IAM aws_iam_role Fornece uma fun\u00e7\u00e3o IAM aws_iam_role_policy_attachment Anexa uma pol\u00edtica IAM gerenciada a uma fun\u00e7\u00e3o IAM aws_redshift_cluster Fornece um recurso de cluster do Redshift aws_redshift_cluster_iam_roles Fornece um recurso de fun\u00e7\u00f5es IAM do cluster do Redshift aws_secretsmanager_secret Fornece um recurso para gerenciar metadados secretos do AWS Secrets Manager aws_secretsmanager_secret_version Fornece um recurso para gerenciar a vers\u00e3o secreta do AWS Secrets Manager, incluindo seu valor secreto aws_security_group_rule Fornece um recurso de regra de grupo de seguran\u00e7a. Representa uma \u00fanica regra de grupo de entrada ou sa\u00edda, que pode ser adicionada a grupos de seguran\u00e7a externos Recursos do Terraform Descri\u00e7\u00e3o random_password Id\u00eantico a random_string com a exce\u00e7\u00e3o de que o resultado \u00e9 tratado como confidencial e, portanto, n\u00e3o \u00e9 exibido na sa\u00edda do console random_pet O recurso random_pet gera nomes de animais de estima\u00e7\u00e3o aleat\u00f3rios que devem ser usados como identificadores exclusivos para outros recursos Ordem da execu\u00e7\u00e3o dos comandos \u00b6 Clique aqui para ir na ordem dos comandos [Extra] Comandos \u00b6 comandos descri\u00e7\u00e3o terraform init Inicializa a configura\u00e7\u00e3o do Terraform terraform plan comando permite visualizar as a\u00e7\u00f5es que o Terraform executaria para modificar sua infraestrutura ou salvar um plano especulativo que pode ser aplicado posteriormente terraform destroy Destr\u00f3i a configura\u00e7\u00e3o provisionada terraform validate Valida sua configura\u00e7\u00e3o terraform plan -out=\"tfplan.out\" Gera um arquivo .out do resultado executado pelo terraform plan terraform apply \"tfplan.out\" Executa modifica\u00e7\u00f5es com base no arquivo marcado terraform plan -destroy -out=\"des.plan\" Gera um arquivo .plan do resultado executado pelo terraform destroy terraform apply \"des.plan\" Executa modifica\u00e7\u00f5es de acordo com o arquivo marcado Check Objetivos \u00b6 Os \u00edcones s\u00e3o os finalizados e os s\u00e3o os em abertos. Cria\u00e7\u00e3o de um remote state Cria\u00e7\u00e3o de um bucket S3 Cria\u00e7\u00e3o de um cluster Redshift Adi\u00e7\u00e3o de entradas cidr Criando grupos e usu\u00e1rios Salvando dados sens\u00edveis no Secrets Manager","title":"Usando Terraform para provisionar infraestrutura"},{"location":"src/engdados/desenvolvimento/aws_terraform/#terraform","text":"","title":"Terraform"},{"location":"src/engdados/desenvolvimento/aws_terraform/#create_remote_state","text":"No Script requisitamos o ID da conta AWS utilizando o profile e region para poder criar o bucket com Id \u00fanico. Adicionamos tags, versionamento e acl private ao provisionamento do bucket S3. main.tf terraform\\01_terraform_remote_state\\main.tf","title":"Provisionando remote state no S3"},{"location":"src/engdados/desenvolvimento/aws_terraform/#recursos-provisionados","text":"Recursos do Provedor Descri\u00e7\u00e3o aws_s3_bucket Fornece um recurso de bucket do S3 aws_s3_bucket_acl Configura\u00e7\u00e3o de ACL no bucket aws_s3_bucket_versioning Configura\u00e7\u00e3o de versionamento no bucket","title":"Recursos provisionados"},{"location":"src/engdados/desenvolvimento/aws_terraform/#order_execute","text":"Abaixo clique em expandir para visualizar a ordem da execu\u00e7\u00e3o dos comandos caso necessite. A mesma ordem ser\u00e1 utilizada nos demais t\u00f3picos. Inicializando a configura\u00e7\u00e3o do Terraform terraform init Validando sua configura\u00e7\u00e3o terraform validate Executando um plano de cria\u00e7\u00e3o terraform plan Ap\u00f3s revisar o plano de cria\u00e7\u00e3o, vamos aplicar e aprovar a execu\u00e7\u00e3o automaticamente terraform apply -auto-approve","title":"Ordem da execu\u00e7\u00e3o dos comandos"},{"location":"src/engdados/desenvolvimento/aws_terraform/#create_bucket","text":"Provisionamento do bucket S3 com adi\u00e7\u00e3o de arquivos em csv e um jpg. \u00c9 utlizado backend e module no script main. O S3 cont\u00e9m ciclo de vida, versionamento, ACL e adi\u00e7\u00e3o de objetos. main.tf terraform\\02_aws_s3_and_files\\main.tf","title":"Provisionando bucket S3 com arquivos"},{"location":"src/engdados/desenvolvimento/aws_terraform/#recursos-provisionados_1","text":"Recursos do Provedor Descri\u00e7\u00e3o aws_s3_bucket Esta funcionalidade \u00e9 para gerenciar o S3 em uma parti\u00e7\u00e3o AWS aws_s3_bucket_acl Fornece um recurso de ACL de bucket do S3 aws_s3_bucket_lifecycle_configuration Fornece um recurso de configura\u00e7\u00e3o independente para a configura\u00e7\u00e3o do ciclo de vida do bucket do S3 aws_s3_bucket_metric Fornece um recurso de configura\u00e7\u00e3o de m\u00e9tricas de bucket do S3 aws_s3_bucket_versioning Fornece um recurso para controlar o controle de vers\u00e3o em um bucket do S3 aws_s3_object Fornece um recurso de objeto S3 Recursos Terraform Descri\u00e7\u00e3o random_pet Gera nomes de animais aleat\u00f3rios que podem ser usados como identificadores exclusivos para outros recursos","title":"Recursos provisionados"},{"location":"src/engdados/desenvolvimento/aws_terraform/#ordem-da-execucao-dos-comandos","text":"Clique aqui para ir na ordem dos comandos","title":"Ordem da execu\u00e7\u00e3o dos comandos"},{"location":"src/engdados/desenvolvimento/aws_terraform/#create_cluster_redshift","text":"O script possui a cria\u00e7\u00e3o do cluster Redshift, cria\u00e7\u00e3o de usu\u00e1rios e grupos para Redshift, adi\u00e7\u00e3o de role e policy, cria\u00e7\u00e3o de secrets manager com adi\u00e7\u00e3o de informa\u00e7\u00f5es criadas no processo para n\u00e3o utilizar as senhas diretamente no c\u00f3digo, adi\u00e7\u00e3o de IP'S para acesso aos recursos provisionados. Pontuarei apenas algumas partes que precisam de modifica\u00e7\u00f5es: main.tf terraform\\03_terraform_remote_state\\main.tf","title":"Provisionando Redshift e outros recursos necess\u00e1rios."},{"location":"src/engdados/desenvolvimento/aws_terraform/#criacao-cluster-redshift","text":"Esse cluster atende as caracteristicas do modelo free tier, caso esteja dentro do prazo. Pode-se modificar master_username e database_name . resource \"aws_redshift_cluster\" \"cluster_reds\" { clus ter _ide nt i f ier = ra n dom_pe t . t his.id da ta base_ na me = \"northwind\" mas ter _user na me = \"aylton\" mas ter _password = ra n dom_password.password [ 0 ] .resul t por t = 5439 n ode_ t ype = \"dc2.large\" clus ter _ t ype = \"single-node\" publicly_accessible = true apply_immedia tel y = true # Necess\u00e1rio para des tr oy do clus ter qua n do t iver s na psho t skip_ f i nal _s na psho t = true # Necess\u00e1rio para des tr oy do clus ter qua n do t iver s na psho t ta gs = var. ta gs }","title":"Cria\u00e7\u00e3o Cluster Redshift"},{"location":"src/engdados/desenvolvimento/aws_terraform/#adi\u00e7\u00e3o-de-entrada-cidr","text":"Foram criadas 3 adi\u00e7\u00f5es de CIDR: Meu IP, Entrada para o dbt e Entrada do Looker Studio. E Fa\u00e7a altera\u00e7\u00e3o para seu IP, sem ele voc\u00ea n\u00e3o conseguir\u00e1 realizar opera\u00e7\u00f5es futuras. Pode-se modificar description e cidr_blocks . \"security_rule\" \"security_rule_dbt\" \"security_rule_lookerstudio\" resource \"aws_security_group_rule\" \"security_rule\" { descrip t io n = \"Redshift Aylton\" t ype = \"ingress\" fr om_por t = 5439 t o_por t = 5439 pro t ocol = \"tcp\" cidr_blocks = [ \"${chomp(data.http.meu_ip.response_body)}/32\" ] # meu IP securi t y_group_id = da ta .aws_securi t y_group.securi t y_group_da ta .id } resource \"aws_security_group_rule\" \"security_rule_dbt\" { descrip t io n = \"DBT 52.45\" t ype = \"ingress\" fr om_por t = 5439 t o_por t = 5439 pro t ocol = \"tcp\" cidr_blocks = [ \"52.45.144.63/32\" , \"54.81.134.249/32\" , \"52.22.161.231/32\" ] # IP db t securi t y_group_id = da ta .aws_securi t y_group.securi t y_group_da ta .id } resource \"aws_security_group_rule\" \"security_rule_lookerstudio\" { descrip t io n = \"looker_studio\" t ype = \"ingress\" fr om_por t = 5439 t o_por t = 5439 pro t ocol = \"tcp\" cidr_blocks = [ \"142.251.74.0/23\" , \"74.125.0.0/16\" ] # IP Looker s tu dio securi t y_group_id = da ta .aws_securi t y_group.securi t y_group_da ta .id } Adicionado dado de captura autom\u00e1tica ip da ta \"http\" \"meu_ip\" { url = \"https://ipv4.icanhazip.com\" } Agora toda vez que seu Ip mudar, voc\u00ea n\u00e3o precisa mais alterar manualmente o n\u00famero do IP. Basta usar o terraform plan para verificar as mudan\u00e7as e terraform apply -auto-approve para aplic\u00e1-las.","title":"Adi\u00e7\u00e3o de entrada CIDR"},{"location":"src/engdados/desenvolvimento/aws_terraform/#cria\u00e7\u00e3o-de-usu\u00e1rios-grupos-e-adi\u00e7\u00e3o-de-usu\u00e1rio-ao-grupo","text":"Recurso Removido Os recursos espec\u00edficos para este t\u00f3pico estavam gerando instabilidades no terraform, ent\u00e3o a cria\u00e7\u00e3o dos usu\u00e1rios, grupos e adi\u00e7\u00e3o de usu\u00e1rios ao grupos foram movidos para o python no t\u00f3pico Cria\u00e7\u00e3o de usu\u00e1rios, grupos e vinculos","title":"Cria\u00e7\u00e3o de usu\u00e1rios, grupos e adi\u00e7\u00e3o de usu\u00e1rio ao grupo"},{"location":"src/engdados/desenvolvimento/aws_terraform/#salvando-usu\u00e1rios-e-senhas-no-secret-manager","text":"Um recurso muito importante quando trata-se de guardar dados mais sens\u00edveis, principalmente quando estamos gerando senhas e logins como \u00e9 caso desse pequeno projeto. O script vai guardar os logins e senhas referente ao Redshift, al\u00e9m de outras informa\u00e7\u00f5es para serem utilizadas no python futuramente. resource \"aws_secretsmanager_secret_version\" \"rd_secret_version\" { secre t _id = aws_secre ts ma na ger_secre t .rd_secre t .id secre t _s tr i n g = jso nen code( { e n gi ne = \"redshift\" da ta _base = aws_redshi ft _clus ter .clus ter _reds.da ta base_ na me user na me = aws_redshi ft _clus ter .clus ter _reds.mas ter _user na me password = aws_redshi ft _clus ter .clus ter _reds.mas ter _password por t = aws_redshi ft _clus ter .clus ter _reds.por t hos t = aws_redshi ft _clus ter .clus ter _reds.e n dpoi nt dbClus ter Ide nt i f ier = aws_redshi ft _clus ter .clus ter _reds.clus ter _ide nt i f ier db t _prod = \"dbt_prod\" db t _prod_password = ra n dom_password.password [ 1 ] .resul t db t _dev = \"dbt_dev\" db t _dev_password = ra n dom_password.password [ 2 ] .resul t looker_user = \"looker\" looker_password = ra n dom_password.password [ 3 ] .resul t } ) }","title":"Salvando usu\u00e1rios e senhas no Secret Manager"},{"location":"src/engdados/desenvolvimento/aws_terraform/#recursos-provisionados_2","text":"Recursos do Provedor Descri\u00e7\u00e3o aws_iam_policy Fornece uma pol\u00edtica IAM aws_iam_role Fornece uma fun\u00e7\u00e3o IAM aws_iam_role_policy_attachment Anexa uma pol\u00edtica IAM gerenciada a uma fun\u00e7\u00e3o IAM aws_redshift_cluster Fornece um recurso de cluster do Redshift aws_redshift_cluster_iam_roles Fornece um recurso de fun\u00e7\u00f5es IAM do cluster do Redshift aws_secretsmanager_secret Fornece um recurso para gerenciar metadados secretos do AWS Secrets Manager aws_secretsmanager_secret_version Fornece um recurso para gerenciar a vers\u00e3o secreta do AWS Secrets Manager, incluindo seu valor secreto aws_security_group_rule Fornece um recurso de regra de grupo de seguran\u00e7a. Representa uma \u00fanica regra de grupo de entrada ou sa\u00edda, que pode ser adicionada a grupos de seguran\u00e7a externos Recursos do Terraform Descri\u00e7\u00e3o random_password Id\u00eantico a random_string com a exce\u00e7\u00e3o de que o resultado \u00e9 tratado como confidencial e, portanto, n\u00e3o \u00e9 exibido na sa\u00edda do console random_pet O recurso random_pet gera nomes de animais de estima\u00e7\u00e3o aleat\u00f3rios que devem ser usados como identificadores exclusivos para outros recursos","title":"Recursos provisionados"},{"location":"src/engdados/desenvolvimento/aws_terraform/#ordem-da-execucao-dos-comandos_1","text":"Clique aqui para ir na ordem dos comandos","title":"Ordem da execu\u00e7\u00e3o dos comandos"},{"location":"src/engdados/desenvolvimento/aws_terraform/#extra-comandos","text":"comandos descri\u00e7\u00e3o terraform init Inicializa a configura\u00e7\u00e3o do Terraform terraform plan comando permite visualizar as a\u00e7\u00f5es que o Terraform executaria para modificar sua infraestrutura ou salvar um plano especulativo que pode ser aplicado posteriormente terraform destroy Destr\u00f3i a configura\u00e7\u00e3o provisionada terraform validate Valida sua configura\u00e7\u00e3o terraform plan -out=\"tfplan.out\" Gera um arquivo .out do resultado executado pelo terraform plan terraform apply \"tfplan.out\" Executa modifica\u00e7\u00f5es com base no arquivo marcado terraform plan -destroy -out=\"des.plan\" Gera um arquivo .plan do resultado executado pelo terraform destroy terraform apply \"des.plan\" Executa modifica\u00e7\u00f5es de acordo com o arquivo marcado","title":"[Extra] Comandos"},{"location":"src/engdados/desenvolvimento/aws_terraform/#check-objetivos","text":"Os \u00edcones s\u00e3o os finalizados e os s\u00e3o os em abertos. Cria\u00e7\u00e3o de um remote state Cria\u00e7\u00e3o de um bucket S3 Cria\u00e7\u00e3o de um cluster Redshift Adi\u00e7\u00e3o de entradas cidr Criando grupos e usu\u00e1rios Salvando dados sens\u00edveis no Secrets Manager","title":" Check Objetivos"},{"location":"src/engdados/desenvolvimento/dbt/","text":"dbt cloud \u00b6 Apresenta\u00e7\u00e3o \u00b6 Aqui ser\u00e1 realizado toda a parte de organiza\u00e7\u00e3o e cria\u00e7\u00e3o das querys utilizando as boas pr\u00e1ticas presentes nas aulas disponibilizadas pelo pr\u00f3prio dbt. Configura\u00e7\u00f5es \u00b6 Configurando um novo projeto \u00b6 Ao iniciar um projeto dbt cloud temos: Name your project : Aqui voc\u00ea fornece um nome ao projeto. Atualmente n\u00e3o sei se h\u00e1 preenchimento autom\u00e1tico, mas antigamente tinha o padr\u00e3o que era Analytics . Choose a warehouse : Aqui voc\u00ea escolhe o warehouse, selecione uma das op\u00e7\u00f5es e pressione next para prosseguir. Configure your environment : Configura\u00e7\u00e3o do ambiente, dependendo do warehouse pode haver mais ou menos op\u00e7\u00f5es. fique atento ao preenchimento. O username ser\u00e1 o dbt_dev e a senha basta copiar dos valores guardados no secrets manager . Setup a Repository : Escolha qual das op\u00e7\u00f5es ser\u00e1 armazenado o c\u00f3digo do projeto, pressione Connect e fa\u00e7a o v\u00edculo. Depois de configurado basta iniciar o projeto. Abaixo um tutorial do Kahan ! tutorial link Configurando conta dbt cloud Kahan Data Solutions Conven\u00e7\u00f5es de nomenclatura \u00b6 Ao trabalhar neste projeto, estabelecemos algumas conven\u00e7\u00f5es para nomear nossos modelos. Fontes (src) referem-se aos dados brutos da tabela que foram constru\u00eddos no warehouse por meio de um processo de carregamento. Staging (stg) refere-se a modelos que s\u00e3o constru\u00eddos diretamente sobre as fontes. Eles t\u00eam um relacionamento um-para-um com as tabelas de origem. Eles s\u00e3o usados para transforma\u00e7\u00f5es muito leves que moldam os dados no que voc\u00ea deseja que sejam. Esses modelos s\u00e3o usados para limpar e padronizar os dados antes de transformar os dados downstream. Nota: Normalmente, eles s\u00e3o materializados como exibi\u00e7\u00f5es. Intermedi\u00e1rio (int) refere-se a quaisquer modelos existentes entre as tabelas de fatos e dimens\u00f5es finais. Eles devem ser constru\u00eddos em modelos de prepara\u00e7\u00e3o em vez de diretamente em fontes para aproveitar a limpeza de dados que foi feita na prepara\u00e7\u00e3o. Fato (fct) refere-se a qualquer dado que represente algo que ocorreu ou est\u00e1 ocorrendo. Os exemplos incluem sess\u00f5es, transa\u00e7\u00f5es, pedidos, hist\u00f3rias, votos. Geralmente s\u00e3o mesas finas e compridas. Dimens\u00e3o (dim) refere-se a dados que representam uma pessoa, lugar ou coisa. Exemplos incluem clientes, produtos, candidatos, edif\u00edcios, funcion\u00e1rios. Nota: A conven\u00e7\u00e3o Fato e Dimens\u00e3o \u00e9 baseada em t\u00e9cnicas de modelagem normalizadas anteriores. Explanando sobre a organiza\u00e7\u00e3o \u00b6 Os exemplos v\u00e3o variar, cada cliente tem uma estrutura diferente de organiza\u00e7\u00e3o sobre os arquivos sql e yaml . renner : Apenas um source contendo todas as tabelas, algumas colunas e tests. Os arquivos stg.sql guardam as querys de consulta das fontes. Os arquivos stg.yml guardam as colunas brutas e descri\u00e7\u00f5es (foco documenta\u00e7\u00e3o). Na pasta mart a subpasta core guarda apenas as dimens\u00f5es, fatos e intermedi\u00e1rios. Enquanto outras subpastas da mart guardam as querys por \u00e1rea. dbt-project \u00b6 N\u00e3o foram feitas grandes altera\u00e7\u00f5es, somente na padroniza\u00e7\u00e3o da materializa\u00e7\u00e3o. O que estiver na mart ser\u00e1 table e o que estiver na staging ser\u00e1 view, mas podem mudar para o que acharem conveniente dbt\\dbt_project.yml ... models: engdados: mart: materialized: table staging: materialized: view ... Models \u00b6 Sobre os models, possuem dois diret\u00f3rios: mart e staging . Mart : No diret\u00f3rio de mart s\u00e3o adicionados os modelos de intermediarios, fatos e dimens\u00f5es. Dependendo do caso podem surgir novos diret\u00f3rios por \u00e1rea. Exemplo: Marketing, Financeiro. Staging : No diret\u00f3rio de staging s\u00e3o adicionados os modelos de configura\u00e7\u00e3o das fontes e prepara\u00e7\u00e3o das fontes. \u00c9 aqui que encontrar\u00e1 os arquivos no formato YML, contendo as configura\u00e7\u00f5es das fontes. Dependendo do caso podem sugir novos diret\u00f3rios por fontes. Exemplo: Salesforce, Stripe, Segment. Staging \u00b6 Em staging temos os arquivos src (source) e stg (staging). Os sources s\u00e3o arquivos em yml contendo configura\u00e7\u00f5es sobre o schema descrito. As estrat\u00e9gias para como organizar as configura\u00e7\u00f5es depende muito, um exemplo \u00e9 c\u00f3digo abaixo: dbt\\models\\staging\\northwind\\renner\\src_renner.yml version : 2 sources : - name : renner description : 'Database do setor de dados' database : northwind schema : \"renner\" tables : - name : employees description : \"informa\u00e7\u00f5es sobre os funcion\u00e1rios\" columns : - name : employeeid tests : - unique - name : categories - name : customers - name : orderdetails - name : orders - name : products - name : shippers - name : suppliers Os arquivos staging podem variar entre sql e possuir um yml . Exemplo: O arquivo stg.yml possui campos de descri\u00e7\u00f5es para adicionar informa\u00e7\u00f5es, assim voc\u00ea deixa somente o \"grosso\" no stg.sql , permitindo uma leitura melhor sobre as querys. Abaixo um exemplo de stg.yml e stg.sql dbt\\models\\staging\\northwind\\netshoes\\stg_netshoes_customers.yml dbt\\models\\staging\\northwind\\netshoes\\stg_netshoes_customers.sql version : 2 models : - name : stg_netshoes_customers description : \"C\u00f3pia da tabela original da costumers do banco northwind\" columns : - name : customerid description : \"Primary key da tabela costumers\" - name : companyname description : \"Nome da empresa integro\" - name : contactname description : \"Nome do respons\u00e1vel\" - name : contacttitle description : \"{{ doc('costumers_contacttitle') }}\" ... {{ config ( materialized = 'table' ) }} with source as ( select * from {{ source ( 'netshoes' , 'customers' ) }} ), customers as ( select customerid , companyname , contactname , contacttitle , address , city , region , postalcode , country , phone , created_at , CAST ( updated_at AS TIMESTAMP ) as updated_at from source ) select * from customers Mart \u00b6 O diret\u00f3rio core possui os clientes , e neles est\u00e3o as dimens\u00f5es, fatos e intermedi\u00e1rios. Modelagem dimens\u00e3o e fato, no exemplo abaixo: dbt\\models\\mart\\core\\renner\\dim_renner_orderdetails.sql dbt\\models\\mart\\core\\renner\\fct_renner_products.sql with orderdetails as ( select odd . orderid , odd . productid , odd . preco_vendido , .... orders . transportadoras , orders . transportadoras_phone , orders . vendedor , ( preco_tabela - preco_vendido ) as diferenca , ( preco_vendido * quantidade_vendida ) as total , (( preco_tabela * quantidade_vendida ) - total ) as desconto , ( date_part ( year , orders . orderdate :: date ) || '-01-01' ):: date as ano from {{ ref ( 'stg_renner_orderdetails' ) }} as odd left join {{ ref ( 'fct_renner_products' ) }} as po ON ( odd . productid = po . productid ) left join {{ ref ( 'fct_renner_orders' ) }} as orders ON ( odd . orderid = orders . orderid ) ) select * from orderdetails with products as ( select products . productid , products . productname as produto , products . unitprice :: decimal ( 10 , 3 ) as preco_tabela , categories . categoryid , categories . categoryname as categoria , suppliers . supplierid , suppliers . companyname as fornecedores , suppliers . contactname as fornecedores_contatos from {{ ref ( 'stg_renner_products' ) }} as products left join {{ ref ( 'stg_renner_categories' ) }} as categories on ( products . categoryid = categories . categoryid ) left join {{ ref ( 'stg_renner_suppliers' ) }} as suppliers on ( products . supplierid = suppliers . supplierid ) ) select * from products Lembre-se que outros diret\u00f3rios podem nascer por \u00e1rea, por exemplo: Venda. Utilizei post_hook e adicionei um grupo espec\u00edfico para a visualiza\u00e7\u00e3o (reporters: looker). dbt\\models\\mart\\venda\\renner\\renner_categorias_mais_vendidas.sql {{ config ( materialized = 'table' , post_hook = [ \" grant usage on schema {{target.schema}} to group reporters; grant select on table {{target.schema}}.renner_categorias_mais_vendidas to group reporters;\" ] ) }} with categorias as ( select categoria , ano , sum ( total ) as total , row_number () over ( partition by ano order by sum ( total ) desc ) as rank_categoria from {{ ref ( \"dim_renner_orderdetails\" ) }} group by categoria , ano ) select * from categorias where rank_categoria <= 5 order by ano , rank_categoria Tests \u00b6 Os testes presentes no projeto s\u00e3o os testes gen\u00e9ricos e os testes singulares . Testes gen\u00e9ricos \u00b6 S\u00e3o feitos nos arquivos yml dentro dos diret\u00f3rios de staging, um exemplo de testes gen\u00e9ricos est\u00e1 aqui abaixo: dbt\\models\\staging\\northwind\\renner\\stg_renner_shippers.yml version : 2 models : - name : dim_renner_orderdetails description : tabela dimens\u00e3o da order details columns : - name : id description : \u00c9 a primary key formada pelos ids das demais tabelas tests : - unique - not_null - name : transportadoras tests : - accepted_values : values : [ 'Speedy Express' , 'United Package' , 'Federal Shipping' ] Testes singulares \u00b6 S\u00e3o adicionados no diret\u00f3rio tests . \u00c9 um arquivo em sql que possui uma query constru\u00edda para projetar valor nenhum e caso retorne algum valor o teste estar\u00e1 como falho. Exemplo abaixo: dbt\\tests\\renner_orderdetails_vendas_positivas.sql select productid , orderid , sum ( preco_vendido ) as vendas from {{ ref ( 'stg_renner_orderdetails' ) }} group by productid , orderid having not ( vendas >= 0 ) H\u00e1 tamb\u00e9m os testes que podem serem introduzidos nas fontes (sources, src). No projeto um desses casos testa se a coluna id de uma tabela \u00e9 \u00fanica. Um exemplo \u00e9 o: dbt\\models\\staging\\northwind\\renner\\src_renner.yml version : 2 sources : - name : renner description : 'Database do setor de dados' database : northwind schema : \"renner\" tables : - name : employees description : \"informa\u00e7\u00f5es sobre os funcion\u00e1rios\" columns : - name : employeeid tests : - unique - name : categories - name : customers - name : orderdetails - name : orders - name : products - name : shippers - name : suppliers Packages \u00b6 No arquivo packages.yml ter\u00e1 o dbt-utils, um pacote com utilidades interessantes. O projeto possui: codegen e dbt_utils . Abaixo deixarei os links. dbt links site get_dbt packages dbt_utils , codegen github dbt-utils Dbt_utils \u00b6 O dbt_utils cont\u00e9m macros que podem serem utilizados no projeto dbt. Ainda em constru\u00e7\u00e3o! Codegen \u00b6 O codegen gera c\u00f3digo dbt e exibe na linha de comando. J\u00e1 imaginou ter que preencher na m\u00e3o um arquivo de source com v\u00e1rias tabelas com/sem colunas? Fora de quest\u00e3o, n\u00e9? Por isso o codegen existe! generate_source \u00b6 Essa macro gera um yaml contendo as informa\u00e7\u00f5es necess\u00e1rias para voc\u00ea copiar e colar no seu arquivo source. Ela cont\u00e9m v\u00e1rios argumentos, aumentando a variedade de formas que podem serem gerados. Gera com tabelas e colunas {{ codegen.generate_source( schema_name=\"c&a\" , database_name=\"northwind\" , generate_columns=True ) }} generate_source Gera um yaml do source F\u00e1cil entendimento V\u00e1rias formas de gerar o source generate_base_model \u00b6 Essa macro gera um sql contendo as informa\u00e7\u00f5es necess\u00e1rias para voc\u00ea copiar e colar no seu arquivo base model. Ela cont\u00e9m v\u00e1rios argumentos, aumentando a variedade de formas que podem serem gerados. Gera query com as colunas da tabela espec\u00edficada {{ codegen.generate_base_model( source_name='c&a' , table_name='customers' ) }} generate_base_model Gera um sql para um modelo base F\u00e1cil entendimento V\u00e1rias formas de gerar o modelo base generate_model_yaml \u00b6 Essa macro gera um yaml contendo os nomes das colunas e descri\u00e7\u00f5es vazias para documenta\u00e7\u00e3o. Ela cont\u00e9m 2 argumentos. Gera yml para tabela ou view espec\u00edficada {{ codegen.generate_model_yaml( model_names= [ 'stg_c&a_customers' ] ) }} generate_model_yaml Gera um yml com colunas e descri\u00e7\u00f5es F\u00e1cil entendimento Check Objetivos \u00b6 Os \u00edcones s\u00e3o os finalizados e os s\u00e3o os em abertos. Configura\u00e7\u00f5es Conven\u00e7\u00f5es de nomenclatura Project Models Testes Packages Jobs Cria\u00e7\u00e3o de Jobs Integra\u00e7\u00e3o com slack e e-mail Documenta\u00e7\u00e3o sobre as querys Utiliza\u00e7\u00e3o de dbt docs Utiliza\u00e7\u00e3o de Seeds Utiliza\u00e7\u00e3o dos macros","title":"Utilizando Dbt cloud"},{"location":"src/engdados/desenvolvimento/dbt/#dbt-cloud","text":"","title":"dbt cloud"},{"location":"src/engdados/desenvolvimento/dbt/#apresentacao","text":"Aqui ser\u00e1 realizado toda a parte de organiza\u00e7\u00e3o e cria\u00e7\u00e3o das querys utilizando as boas pr\u00e1ticas presentes nas aulas disponibilizadas pelo pr\u00f3prio dbt.","title":"Apresenta\u00e7\u00e3o"},{"location":"src/engdados/desenvolvimento/dbt/#configura\u00e7\u00f5es","text":"","title":"Configura\u00e7\u00f5es"},{"location":"src/engdados/desenvolvimento/dbt/#configurando-um-novo-projeto","text":"Ao iniciar um projeto dbt cloud temos: Name your project : Aqui voc\u00ea fornece um nome ao projeto. Atualmente n\u00e3o sei se h\u00e1 preenchimento autom\u00e1tico, mas antigamente tinha o padr\u00e3o que era Analytics . Choose a warehouse : Aqui voc\u00ea escolhe o warehouse, selecione uma das op\u00e7\u00f5es e pressione next para prosseguir. Configure your environment : Configura\u00e7\u00e3o do ambiente, dependendo do warehouse pode haver mais ou menos op\u00e7\u00f5es. fique atento ao preenchimento. O username ser\u00e1 o dbt_dev e a senha basta copiar dos valores guardados no secrets manager . Setup a Repository : Escolha qual das op\u00e7\u00f5es ser\u00e1 armazenado o c\u00f3digo do projeto, pressione Connect e fa\u00e7a o v\u00edculo. Depois de configurado basta iniciar o projeto. Abaixo um tutorial do Kahan ! tutorial link Configurando conta dbt cloud Kahan Data Solutions","title":"Configurando um novo projeto"},{"location":"src/engdados/desenvolvimento/dbt/#conven\u00e7\u00f5es-de-nomenclatura","text":"Ao trabalhar neste projeto, estabelecemos algumas conven\u00e7\u00f5es para nomear nossos modelos. Fontes (src) referem-se aos dados brutos da tabela que foram constru\u00eddos no warehouse por meio de um processo de carregamento. Staging (stg) refere-se a modelos que s\u00e3o constru\u00eddos diretamente sobre as fontes. Eles t\u00eam um relacionamento um-para-um com as tabelas de origem. Eles s\u00e3o usados para transforma\u00e7\u00f5es muito leves que moldam os dados no que voc\u00ea deseja que sejam. Esses modelos s\u00e3o usados para limpar e padronizar os dados antes de transformar os dados downstream. Nota: Normalmente, eles s\u00e3o materializados como exibi\u00e7\u00f5es. Intermedi\u00e1rio (int) refere-se a quaisquer modelos existentes entre as tabelas de fatos e dimens\u00f5es finais. Eles devem ser constru\u00eddos em modelos de prepara\u00e7\u00e3o em vez de diretamente em fontes para aproveitar a limpeza de dados que foi feita na prepara\u00e7\u00e3o. Fato (fct) refere-se a qualquer dado que represente algo que ocorreu ou est\u00e1 ocorrendo. Os exemplos incluem sess\u00f5es, transa\u00e7\u00f5es, pedidos, hist\u00f3rias, votos. Geralmente s\u00e3o mesas finas e compridas. Dimens\u00e3o (dim) refere-se a dados que representam uma pessoa, lugar ou coisa. Exemplos incluem clientes, produtos, candidatos, edif\u00edcios, funcion\u00e1rios. Nota: A conven\u00e7\u00e3o Fato e Dimens\u00e3o \u00e9 baseada em t\u00e9cnicas de modelagem normalizadas anteriores.","title":"Conven\u00e7\u00f5es de nomenclatura"},{"location":"src/engdados/desenvolvimento/dbt/#explanando-sobre-a-organiza\u00e7\u00e3o","text":"Os exemplos v\u00e3o variar, cada cliente tem uma estrutura diferente de organiza\u00e7\u00e3o sobre os arquivos sql e yaml . renner : Apenas um source contendo todas as tabelas, algumas colunas e tests. Os arquivos stg.sql guardam as querys de consulta das fontes. Os arquivos stg.yml guardam as colunas brutas e descri\u00e7\u00f5es (foco documenta\u00e7\u00e3o). Na pasta mart a subpasta core guarda apenas as dimens\u00f5es, fatos e intermedi\u00e1rios. Enquanto outras subpastas da mart guardam as querys por \u00e1rea.","title":"Explanando sobre a organiza\u00e7\u00e3o"},{"location":"src/engdados/desenvolvimento/dbt/#dbt-project","text":"N\u00e3o foram feitas grandes altera\u00e7\u00f5es, somente na padroniza\u00e7\u00e3o da materializa\u00e7\u00e3o. O que estiver na mart ser\u00e1 table e o que estiver na staging ser\u00e1 view, mas podem mudar para o que acharem conveniente dbt\\dbt_project.yml ... models: engdados: mart: materialized: table staging: materialized: view ...","title":"dbt-project"},{"location":"src/engdados/desenvolvimento/dbt/#dbt-models","text":"Sobre os models, possuem dois diret\u00f3rios: mart e staging . Mart : No diret\u00f3rio de mart s\u00e3o adicionados os modelos de intermediarios, fatos e dimens\u00f5es. Dependendo do caso podem surgir novos diret\u00f3rios por \u00e1rea. Exemplo: Marketing, Financeiro. Staging : No diret\u00f3rio de staging s\u00e3o adicionados os modelos de configura\u00e7\u00e3o das fontes e prepara\u00e7\u00e3o das fontes. \u00c9 aqui que encontrar\u00e1 os arquivos no formato YML, contendo as configura\u00e7\u00f5es das fontes. Dependendo do caso podem sugir novos diret\u00f3rios por fontes. Exemplo: Salesforce, Stripe, Segment.","title":"Models"},{"location":"src/engdados/desenvolvimento/dbt/#staging","text":"Em staging temos os arquivos src (source) e stg (staging). Os sources s\u00e3o arquivos em yml contendo configura\u00e7\u00f5es sobre o schema descrito. As estrat\u00e9gias para como organizar as configura\u00e7\u00f5es depende muito, um exemplo \u00e9 c\u00f3digo abaixo: dbt\\models\\staging\\northwind\\renner\\src_renner.yml version : 2 sources : - name : renner description : 'Database do setor de dados' database : northwind schema : \"renner\" tables : - name : employees description : \"informa\u00e7\u00f5es sobre os funcion\u00e1rios\" columns : - name : employeeid tests : - unique - name : categories - name : customers - name : orderdetails - name : orders - name : products - name : shippers - name : suppliers Os arquivos staging podem variar entre sql e possuir um yml . Exemplo: O arquivo stg.yml possui campos de descri\u00e7\u00f5es para adicionar informa\u00e7\u00f5es, assim voc\u00ea deixa somente o \"grosso\" no stg.sql , permitindo uma leitura melhor sobre as querys. Abaixo um exemplo de stg.yml e stg.sql dbt\\models\\staging\\northwind\\netshoes\\stg_netshoes_customers.yml dbt\\models\\staging\\northwind\\netshoes\\stg_netshoes_customers.sql version : 2 models : - name : stg_netshoes_customers description : \"C\u00f3pia da tabela original da costumers do banco northwind\" columns : - name : customerid description : \"Primary key da tabela costumers\" - name : companyname description : \"Nome da empresa integro\" - name : contactname description : \"Nome do respons\u00e1vel\" - name : contacttitle description : \"{{ doc('costumers_contacttitle') }}\" ... {{ config ( materialized = 'table' ) }} with source as ( select * from {{ source ( 'netshoes' , 'customers' ) }} ), customers as ( select customerid , companyname , contactname , contacttitle , address , city , region , postalcode , country , phone , created_at , CAST ( updated_at AS TIMESTAMP ) as updated_at from source ) select * from customers","title":"Staging"},{"location":"src/engdados/desenvolvimento/dbt/#mart","text":"O diret\u00f3rio core possui os clientes , e neles est\u00e3o as dimens\u00f5es, fatos e intermedi\u00e1rios. Modelagem dimens\u00e3o e fato, no exemplo abaixo: dbt\\models\\mart\\core\\renner\\dim_renner_orderdetails.sql dbt\\models\\mart\\core\\renner\\fct_renner_products.sql with orderdetails as ( select odd . orderid , odd . productid , odd . preco_vendido , .... orders . transportadoras , orders . transportadoras_phone , orders . vendedor , ( preco_tabela - preco_vendido ) as diferenca , ( preco_vendido * quantidade_vendida ) as total , (( preco_tabela * quantidade_vendida ) - total ) as desconto , ( date_part ( year , orders . orderdate :: date ) || '-01-01' ):: date as ano from {{ ref ( 'stg_renner_orderdetails' ) }} as odd left join {{ ref ( 'fct_renner_products' ) }} as po ON ( odd . productid = po . productid ) left join {{ ref ( 'fct_renner_orders' ) }} as orders ON ( odd . orderid = orders . orderid ) ) select * from orderdetails with products as ( select products . productid , products . productname as produto , products . unitprice :: decimal ( 10 , 3 ) as preco_tabela , categories . categoryid , categories . categoryname as categoria , suppliers . supplierid , suppliers . companyname as fornecedores , suppliers . contactname as fornecedores_contatos from {{ ref ( 'stg_renner_products' ) }} as products left join {{ ref ( 'stg_renner_categories' ) }} as categories on ( products . categoryid = categories . categoryid ) left join {{ ref ( 'stg_renner_suppliers' ) }} as suppliers on ( products . supplierid = suppliers . supplierid ) ) select * from products Lembre-se que outros diret\u00f3rios podem nascer por \u00e1rea, por exemplo: Venda. Utilizei post_hook e adicionei um grupo espec\u00edfico para a visualiza\u00e7\u00e3o (reporters: looker). dbt\\models\\mart\\venda\\renner\\renner_categorias_mais_vendidas.sql {{ config ( materialized = 'table' , post_hook = [ \" grant usage on schema {{target.schema}} to group reporters; grant select on table {{target.schema}}.renner_categorias_mais_vendidas to group reporters;\" ] ) }} with categorias as ( select categoria , ano , sum ( total ) as total , row_number () over ( partition by ano order by sum ( total ) desc ) as rank_categoria from {{ ref ( \"dim_renner_orderdetails\" ) }} group by categoria , ano ) select * from categorias where rank_categoria <= 5 order by ano , rank_categoria","title":"Mart"},{"location":"src/engdados/desenvolvimento/dbt/#dbt-tests","text":"Os testes presentes no projeto s\u00e3o os testes gen\u00e9ricos e os testes singulares .","title":"Tests"},{"location":"src/engdados/desenvolvimento/dbt/#testes-genericos","text":"S\u00e3o feitos nos arquivos yml dentro dos diret\u00f3rios de staging, um exemplo de testes gen\u00e9ricos est\u00e1 aqui abaixo: dbt\\models\\staging\\northwind\\renner\\stg_renner_shippers.yml version : 2 models : - name : dim_renner_orderdetails description : tabela dimens\u00e3o da order details columns : - name : id description : \u00c9 a primary key formada pelos ids das demais tabelas tests : - unique - not_null - name : transportadoras tests : - accepted_values : values : [ 'Speedy Express' , 'United Package' , 'Federal Shipping' ]","title":"Testes gen\u00e9ricos"},{"location":"src/engdados/desenvolvimento/dbt/#testes-singulares","text":"S\u00e3o adicionados no diret\u00f3rio tests . \u00c9 um arquivo em sql que possui uma query constru\u00edda para projetar valor nenhum e caso retorne algum valor o teste estar\u00e1 como falho. Exemplo abaixo: dbt\\tests\\renner_orderdetails_vendas_positivas.sql select productid , orderid , sum ( preco_vendido ) as vendas from {{ ref ( 'stg_renner_orderdetails' ) }} group by productid , orderid having not ( vendas >= 0 ) H\u00e1 tamb\u00e9m os testes que podem serem introduzidos nas fontes (sources, src). No projeto um desses casos testa se a coluna id de uma tabela \u00e9 \u00fanica. Um exemplo \u00e9 o: dbt\\models\\staging\\northwind\\renner\\src_renner.yml version : 2 sources : - name : renner description : 'Database do setor de dados' database : northwind schema : \"renner\" tables : - name : employees description : \"informa\u00e7\u00f5es sobre os funcion\u00e1rios\" columns : - name : employeeid tests : - unique - name : categories - name : customers - name : orderdetails - name : orders - name : products - name : shippers - name : suppliers","title":"Testes singulares"},{"location":"src/engdados/desenvolvimento/dbt/#packages","text":"No arquivo packages.yml ter\u00e1 o dbt-utils, um pacote com utilidades interessantes. O projeto possui: codegen e dbt_utils . Abaixo deixarei os links. dbt links site get_dbt packages dbt_utils , codegen github dbt-utils","title":"Packages"},{"location":"src/engdados/desenvolvimento/dbt/#dbt-utils","text":"O dbt_utils cont\u00e9m macros que podem serem utilizados no projeto dbt. Ainda em constru\u00e7\u00e3o!","title":"Dbt_utils"},{"location":"src/engdados/desenvolvimento/dbt/#codegen","text":"O codegen gera c\u00f3digo dbt e exibe na linha de comando. J\u00e1 imaginou ter que preencher na m\u00e3o um arquivo de source com v\u00e1rias tabelas com/sem colunas? Fora de quest\u00e3o, n\u00e9? Por isso o codegen existe!","title":"Codegen"},{"location":"src/engdados/desenvolvimento/dbt/#generate-source","text":"Essa macro gera um yaml contendo as informa\u00e7\u00f5es necess\u00e1rias para voc\u00ea copiar e colar no seu arquivo source. Ela cont\u00e9m v\u00e1rios argumentos, aumentando a variedade de formas que podem serem gerados. Gera com tabelas e colunas {{ codegen.generate_source( schema_name=\"c&a\" , database_name=\"northwind\" , generate_columns=True ) }} generate_source Gera um yaml do source F\u00e1cil entendimento V\u00e1rias formas de gerar o source","title":"generate_source"},{"location":"src/engdados/desenvolvimento/dbt/#generate-base-model","text":"Essa macro gera um sql contendo as informa\u00e7\u00f5es necess\u00e1rias para voc\u00ea copiar e colar no seu arquivo base model. Ela cont\u00e9m v\u00e1rios argumentos, aumentando a variedade de formas que podem serem gerados. Gera query com as colunas da tabela espec\u00edficada {{ codegen.generate_base_model( source_name='c&a' , table_name='customers' ) }} generate_base_model Gera um sql para um modelo base F\u00e1cil entendimento V\u00e1rias formas de gerar o modelo base","title":"generate_base_model"},{"location":"src/engdados/desenvolvimento/dbt/#generate-model-yml","text":"Essa macro gera um yaml contendo os nomes das colunas e descri\u00e7\u00f5es vazias para documenta\u00e7\u00e3o. Ela cont\u00e9m 2 argumentos. Gera yml para tabela ou view espec\u00edficada {{ codegen.generate_model_yaml( model_names= [ 'stg_c&a_customers' ] ) }} generate_model_yaml Gera um yml com colunas e descri\u00e7\u00f5es F\u00e1cil entendimento","title":"generate_model_yaml"},{"location":"src/engdados/desenvolvimento/dbt/#check-objetivos","text":"Os \u00edcones s\u00e3o os finalizados e os s\u00e3o os em abertos. Configura\u00e7\u00f5es Conven\u00e7\u00f5es de nomenclatura Project Models Testes Packages Jobs Cria\u00e7\u00e3o de Jobs Integra\u00e7\u00e3o com slack e e-mail Documenta\u00e7\u00e3o sobre as querys Utiliza\u00e7\u00e3o de dbt docs Utiliza\u00e7\u00e3o de Seeds Utiliza\u00e7\u00e3o dos macros","title":" Check Objetivos"},{"location":"src/engdados/desenvolvimento/desenvolvimento_lookerstudio/","text":"Em Constru\u00e7\u00e3o \u00b6","title":"Utilizando Looker Studio"},{"location":"src/engdados/desenvolvimento/desenvolvimento_lookerstudio/#em-construcao","text":"","title":"Em Constru\u00e7\u00e3o"},{"location":"src/engdados/desenvolvimento/python/","text":"Python \u00b6 Introdu\u00e7\u00e3o \u00b6 Pensando em aumentar a legibilidade, manuten\u00e7\u00e3o e organiza\u00e7\u00e3o do c\u00f3digo. O c\u00f3digo em python est\u00e1 separado em 3 partes: python\\engdados\\s3_redshift_load_files.py : Script main, contendo de forma resumida o c\u00f3digo sem detalhes das fun\u00e7\u00f5es. python\\engdados\\mod_terraform.py : m\u00f3dulo contendo algumas poucas fun\u00e7\u00f5es para auxiliar com os arquivos do terraform. python\\engdados\\mod_aws.py : m\u00f3dulo contendo algumas fun\u00e7\u00f5es para auxiliar nos recursos da AWS. M\u00f3dulos \u00b6 Nos trechos abaixo, as fun\u00e7\u00f5es v\u00e3o apresentar os m\u00f3dulos na frente, por exemplo: m_tf.get_path_s3() . m\u00f3dulo abrevia\u00e7\u00e3o mod_terraform m_tf mod_aws m_aws Imports \u00b6 Imports utilizados no desenvolvimento. Sobre a instala\u00e7\u00e3o dos imports, favor verificar o t\u00f3pico requirements . Caso tenha dificuldades em ler conte\u00fados em ingl\u00eas, sugiro utilizar o navegador google chrome para visualiza\u00e7\u00e3o das documenta\u00e7\u00f5es. Imports descri\u00e7\u00e3o documenta\u00e7\u00e3o boto3 AWS SDK for Python (Boto3) para criar, configurar e gerenciar servi\u00e7os da AWS AWS SDK for Python (Boto3) pandas Ferramenta de an\u00e1lise e manipula\u00e7\u00e3o de dados de c\u00f3digo aberto r\u00e1pida, poderosa, flex\u00edvel e f\u00e1cil de usar, constru\u00edda sobre a linguagem de programa\u00e7\u00e3o Python Guia do usu\u00e1rio io O m\u00f3dulo fornece as principais facilidades do Python para lidar com v\u00e1rios tipos de E/S io Python json A biblioteca json nos permite poder analisar JSON json Python psycopg2 Adaptador de banco de dados PostgreSQL Official Psycopg logging Este m\u00f3dulo define fun\u00e7\u00f5es e classes que implementam um sistema flex\u00edvel de registro de eventos para aplicativos e bibliotecas. logging Python Requirements \u00b6 Requerimentos utilizados e que necessitam de instala\u00e7\u00e3o. python\\requirements.txt # Requerimentos # Bibliotecas de desenvolvimento pandas # Manipula\u00e7\u00e3o e an\u00e1lise de dados boto3 # Integra\u00e7\u00e3o aos servi\u00e7os AWS psycopg2 # Adaptador do Banco de dados PostgreSQL # Sem pacotes com vers\u00f5es espec\u00edficas Como fa\u00e7o para instalar os requerimentos? Para instala\u00e7\u00e3o de requirements utilize o seguinte comando: pip install -r requirements.txt Ambiente virtual \u00b6 Sobre comandos de cria\u00e7\u00e3o, ativa\u00e7\u00e3o e desativa\u00e7\u00e3o de ambiente virtual. Caso j\u00e1 tenha conhecimento, basta pular para o conex\u00e3o aws . Como usar o ambiente virtual? criar: python - m venv . venv ativar: . \\ . venv \\ Scripts \\ Activate desativar: deactivate Conex\u00e3o AWS \u00b6 Para a conex\u00e3o estarei utilizando o boto3 . Estarei utilizando o profile_name para realizar a sess\u00e3o e posteriomente o cliente. Necessita da CLI AWS s3_redshift_load_files.py # Conex\u00e3o ao Redshift \"\"\"Accessing the S3 buckets using boto3 client\"\"\" session = boto3 . Session ( profile_name = 'ayltonaguiar' ) s3_client = session . client ( 's3' ) s3 = session . resource ( 's3' ) Pegando os arquivos terraform.tfstate \u00b6 Fiz a leitura buscando em 2 lugares diferentes. A primeira parte \u00e9 identifica\u00e7\u00e3o v\u00e1lida dos arquivos (verificar se existem), ent\u00e3o foram criadas 2 fun\u00e7\u00f5es com buscas de caminhos espec\u00edficos: m_tf.get_path_s3() e m_tf.get_path_redshift() . Ambas retornam os caminhos (path) dos arquivos. A segunda parte \u00e9 a leitura dos arquivos identificados. Foram criadas outras 2 fun\u00e7\u00f5es com par\u00e2metros espec\u00edficos para cada uma: m_tf.read_s3_tfstate_backend() e m_tf.read_redshift_tfstate() . Ambas retornam informa\u00e7\u00f5es, por exemplo: regi\u00e3o do redshift, regi\u00e3o do bucket backend (remote state), etc. python\\engdados\\s3_redshift_load_files.py # 1- Pega o diret\u00f3rio dos arquivos, valida e guarda as informa\u00e7\u00f5es b\u00e1sicas em novas vari\u00e1veis path_s3 = m_tf . get_path_s3 () path_redshift = m_tf . get_path_redshift () ## 1.1- s3 backend_details = m_tf . read_s3_tfstate_backend ( f \" { path_s3 } \" ) backend_bucket , backend_region , backend_key = backend_details ## 1.2- redshift redshift_details = m_tf . read_redshift_tfstate ( f \" { path_redshift } \" ) redshift_iam_arn , redshift_secrete_name , redshift_region_name = redshift_details code function get_path_s3() python\\engdados\\mod_terraform.py 13 14 15 16 17 18 def get_path_s3 (): # captura do arquivo terraform.state path_s3_tfstate = Path () . absolute () . parent . parent . joinpath ( 'terraform' , '02_aws_s3_and_files' , '.terraform' , 'terraform.tfstate' ) # valida\u00e7\u00e3o do path path_validate ( path_s3_tfstate , get_path_s3 . __name__ ) return path_s3_tfstate code function get_path_redshift() python\\engdados\\mod_terraform.py 20 21 22 23 24 25 def get_path_redshift (): # captura do arquivo terraform.state path_redshift_tfstate = Path () . absolute () . parent . parent . joinpath ( 'terraform' , '03_aws_redshift' , 'terraform.tfstate' ) # valida\u00e7\u00e3o do path path_validate ( path_redshift_tfstate , get_path_redshift . __name__ ) return path_redshift_tfstate code function read_s3_tfstate_backend() python\\engdados\\mod_terraform.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def read_s3_tfstate_backend ( path ): try : # Captura as informa\u00e7\u00f5es pelo backend do tfstate with open ( path ) as file_name : s3_details_backend = json . load ( file_name ) backend_bucket = s3_details_backend [ 'backend' ][ 'config' ] . get ( 'bucket' ) backend_region = s3_details_backend [ 'backend' ][ 'config' ] . get ( 'region' ) backend_key = s3_details_backend [ 'backend' ][ 'config' ] . get ( 'key' ) print ( '' , \"############ S3 Backend ##########\" , s3_details_backend [ 'backend' ][ 'config' ], sep = ' \\n ' ) return [ backend_bucket , backend_region , backend_key ] except Exception as e : logging . error ( e ) return False code function read_redshift_tfstate() python\\engdados\\mod_terraform.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def read_redshift_tfstate ( path ): # Captura dos outputs criados no arquivo terraform.tfstate da pasta aws_redshift try : with open ( path ) as rd_terraform : rd_terraform_json = json . load ( rd_terraform ) redshift_iam_arn = rd_terraform_json [ 'outputs' ] . get ( 'iam_role_arn' ) . get ( 'value' ) redshift_secrete_name = rd_terraform_json [ 'outputs' ] . get ( 'secrete_name' ) . get ( 'value' ) redshift_region_name = rd_terraform_json [ 'outputs' ] . get ( 'region_name' ) . get ( 'value' ) print ( '' , \"####### Outputs redshift.tfstate ##########\" , redshift_iam_arn , redshift_secrete_name , redshift_region_name , sep = ' \\n ' ) return [ redshift_iam_arn , redshift_secrete_name , redshift_region_name ] except Exception as e : logging . error ( e ) return False Pegar arquivos armazenados no bucket S3 \u00b6 Para identificar objetos no S3, precisamos do nome do bucket e dos arquivos a serem lidos. Para facilitar esse processo, foi criada a fun\u00e7\u00e3o m_aws.get_bucket() . Essa fun\u00e7\u00e3o precisa de informa\u00e7\u00f5es como o nome do bucket compartilhado, a chave do bucket backend e a conex\u00e3o do cliente s3. Ela retornar\u00e1 o nome do bucket que armazena os objetos. A fun\u00e7\u00e3o m_aws.get_csv_s3() verifica e lista os objetos com extens\u00e3o '.csv' ou '.CSV', retornando a lista de objetos. python\\engdados\\s3_redshift_load_files.py # 2- seleciona o objeto do bucket/key fornecido e pega o valor do output 'bucket-name' bucket_name = m_aws . get_bucket ( s3_client , backend_bucket , backend_key ) # 3- Listagem de objetos do bucket fornecido list_objects = m_aws . get_csv_s3 ( s3_client , bucket_name ) code function get_bucket() python\\engdados\\mod_aws.py 8 9 10 11 12 13 14 15 16 17 18 19 20 def get_bucket ( s3_client , backend_bucket , backend_key ): # Identificar bucket remote state try : s3_object = s3_client . get_object ( Bucket = backend_bucket , Key = backend_key ) object_file = s3_object [ \"Body\" ] . read () . decode () parsed_object = json . loads ( object_file ) bucket_name = parsed_object [ 'outputs' ] . get ( 'bucket-name' ) . get ( 'value' ) print ( '' , \"####### Bucket Name Object S3 ##########\" , bucket_name , sep = ' \\n ' ) except Exception as e : logging . error ( e ) return False return bucket_name code function get_csv_s3() python\\engdados\\mod_aws.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def get_csv_s3 ( s3_client , bucket_name ): # Lista os Objetos do bucket setado e armazena na vari\u00e1vel s3_objects try : s3_list_obj = s3_client . list_objects ( Bucket = bucket_name ) s3_objects = [] for obj in s3_list_obj [ 'Contents' ]: if obj [ 'Key' ] . endswith ( 'csv' ) or obj [ 'Key' ] . endswith ( 'CSV' ): s3_objects . append ( obj [ 'Key' ]) print ( '' , \"####### Objects S3 ##########\" , s3_objects [ 0 : 10 ], sep = ' \\n ' ) except Exception as e : logging . error ( e ) return False return s3_objects Pegar dados armazenados no Secrets Manager \u00b6 Para dar prosseguimento precisamos recuperar alguns dados sens\u00edveis adicionados no secrets manager, ent\u00e3o foi criado a fun\u00e7\u00e3o m_aws.get_secrets_redshift() com objetivo de recuperar essas informa\u00e7\u00f5es e armazen\u00e1-las para o pr\u00f3ximo passo. A conex\u00e3o com banco do redshift \u00e9 feita, nesse projeto, pelo uso do psycopg2. Utilizando as informa\u00e7\u00f5es recuperadas pela fun\u00e7\u00e3o m_aws.get_secrets_redshift() . python\\engdados\\s3_redshift_load_files.py # 4- Acessando e pegando os valores guardados do Redshift no Secrets Manager client_secret = session . client ( 'secretsmanager' , region_name = redshift_region_name ) secrets_manager_details = m_aws . get_secrets_redshift ( client_secret , redshift_secrete_name ) redshift_db_name , redshift_db_user , redshift_db_password , redshift_db_port , redshift_db_host = secrets_manager_details # 5- conex\u00e3o ao banco rd_con = psycopg2 . connect ( host = redshift_db_host , database = redshift_db_name , user = redshift_db_user , password = redshift_db_password , port = redshift_db_port ) rd_con . autocommit = True cur = rd_con . cursor () code function get_secrets_redshift() python\\engdados\\mod_aws.py 40 41 42 43 44 45 46 47 48 49 def get_secrets_redshift ( client_secret , redshift_secrete_name ): try : # Captura dos segredos do redshift na AWS Secrets get_secret_value_response = client_secret . get_secret_value ( SecretId = redshift_secrete_name ) secret_json = json . loads ( get_secret_value_response [ 'SecretString' ]) except Exception as e : logging . error ( e ) return False return secret_json Cria\u00e7\u00e3o de usu\u00e1rios, grupos e vinculos \u00b6 A fun\u00e7\u00e3o m_aws.create_users_redshift() tem como objetivo criar os usu\u00e1rios, grupos e realizar vinculos entre as partes. Por \u00faltimo a limpeza do arquivo com dados do secrets manager. python\\engdados\\s3_redshift_load_files.py ## 5.1 Criando os usu\u00e1rios dos grupos loaders, transformers e reporters m_aws . create_users_redshift ( cur , secrets_manager_json ) ## 5.2 Apagando as informa\u00e7\u00f5es do secrets del secrets_manager_json code function create_users_redshift() python\\engdados\\mod_aws.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def create_users_redshift ( cur , secret_json ): try : # Usu\u00e1rios dos grupos - Falta melhorar essa parte bem hardcode e.e Deus me perdoe, mas \u00e9 isso a\u00ed. loaders = [] reporters = [] transformers = [] # Pegando usu\u00e1rios e senhas dos loaders, transformers e reporters for key , value in secret_json . items (): if 'loaders' in key : loaders . append ( value ) elif 'reporters' in key : reporters . append ( value ) elif 'transformers' in key : transformers . append ( value ) # Gerando novas listas a cada 2 items na lista original loaders = [ loaders [ i : i + 2 ] for i in range ( 0 , len ( loaders ), 2 )] reporters = [ reporters [ i : i + 2 ] for i in range ( 0 , len ( reporters ), 2 )] transformers = [ transformers [ i : i + 2 ] for i in range ( 0 , len ( transformers ), 2 )] groups = loaders , reporters , transformers group_names = [ \"loaders\" , \"reporters\" , \"transformers\" ] # Cria\u00e7\u00e3o dos grupos for group in group_names : cur . execute ( f \"CREATE GROUP { group } ;\" ) # Criando Usu\u00e1rios com senhas (0=nome do usu\u00e1rio, 1=senha) for group in groups : for user in group : cur . execute ( f \"create user { user [ 0 ] } with password ' { user [ 1 ] } ';\" ) # Adi\u00e7\u00e3o dos usu\u00e1rios aos grupos for idx , group in enumerate ( groups ): for idx2 , user in enumerate ( group ): cur . execute ( f \"alter group { group_names [ idx ] } add user { user [ 0 ] } ;\" ) except Exception as e : logging . error ( e ) return False Permiss\u00f5es aos grupos e listagem de pastas \u00b6 Lembra que criamos os usu\u00e1rios e grupos de usu\u00e1rios? Ent\u00e3o, estaremos realizando a primeira parte das permiss\u00f5es para os usu\u00e1rios criados, ap\u00f3s essa etapa ser\u00e1 realizado a prepara\u00e7\u00e3o para os nomes das pastas. Para adi\u00e7\u00e3o de permiss\u00f5es, a fun\u00e7\u00e3o m_aws.give_permissions_database() receber\u00e1 a conex\u00e3o, o nome do banco de dados e nome do grupo. A fun\u00e7\u00e3o m_aws.get_folder() pega a primeira parte do diret\u00f3rio da lista de objetos e armazena em SCHEMAS_REDSHIFT . python\\engdados\\s3_redshift_load_files.py # 6- Permiss\u00e3o Create para loaders, select para transformers no banco de dados m_aws . give_permissions_database ( cur , redshift_db_name , groups_redshift ) # 7- pegar o primeiro diret\u00f3rio e adiciona-lo a uma lista de schema. schemas_redshift = [] schemas_redshift = m_aws . get_folder ( list_objects ) code function give_permissions_database() python\\engdados\\mod_aws.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def give_permissions_database ( cur , redshift_db_name , groups ): try : group_exception = [ 'reporters' ] groups_count = group_exception # Executando permiss\u00e3o por grupo n\u00e3o presente em 'groups_count' for group in groups . keys (): if group not in groups_count : cur . execute ( f ''' grant create on database { redshift_db_name } to group { group } ; grant select on all tables in schema information_schema to group { group } ; grant select on all tables in schema pg_catalog to group { group } ; ''' ) groups_count . append ( group ) except Exception as e : logging . error ( e ) return False code function get_folder() python\\engdados\\mod_aws.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def get_folder ( list_objects ): # Cria\u00e7\u00e3o da primeira parte do diret\u00f3rio como schema no banco de dados redshift try : folders = [] for folder in list_objects : folder = folder . split ( '/' , 1 )[ 0 ] if ( folder not in folders ): # Adiciona apenas as pastas diferentes folders . append ( folder . split ( '/' , 1 )[ 0 ]) return folders except Exception as e : logging . error ( e ) return False #print(folders) Cria\u00e7\u00e3o e Permiss\u00f5es para Schemas \u00b6 Nessa etapa criaremos os schemas e adicionaremos as permiss\u00f5es para os usu\u00e1rios do grupo 'transformers'. A fun\u00e7\u00e3o m_aws.create_schema_redshift() cria os schemas com base nos diret\u00f3rios armazenados em schemas_redshift . Os Tranformadores precisam de acesso aos schemas criados, pois v\u00e3o trabalhar com todas as tabelas existentes. A fun\u00e7\u00e3o m_aws.give_permission_schemas() visa permitir ao grupo selecionado tenha permiss\u00f5es, al\u00e9m de fazer com que o redshift_db_user ganhe privil\u00e9gios para as novas tabelas criadas. python\\engdados\\s3_redshift_load_files.py # 8- Cria\u00e7\u00e3o dos schemas caso n\u00e3o exista m_aws . create_schema_redshift ( cur , schemas_redshift , redshift_db_user ) # 9- Permiss\u00e3o para o grupo transformers sobre os schemas criados m_aws . give_permission_schemas ( cur , schemas_redshift , redshift_db_user , group_transformers ) code function create_schema_redshift() python\\engdados\\mod_aws.py 165 166 167 168 169 170 171 172 173 174 175 def create_schema_redshift ( cur , folders , redshift_db_user ): # Cria\u00e7\u00e3o da primeira parte do diret\u00f3rio como schema no banco de dados redshift try : for schema in folders : print ( '' , \"####### Comando executado: ##########\" , sep = ' \\n ' ) print ( f 'CREATE SCHEMA IF NOT EXISTS \" { schema } \" AUTHORIZATION { redshift_db_user } ;' ) cur . execute ( f 'CREATE SCHEMA IF NOT EXISTS \" { schema } \" AUTHORIZATION { redshift_db_user } ;' ) # vai criar a pasta entre \"\", exemplo: \"c&a\" except Exception as e : logging . error ( e ) return False return True code function give_permission_schemas() python\\engdados\\mod_aws.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def give_permission_schemas ( cur , folders , redshift_db_user , group ): try : for key , value in group . items (): if value == 2 : # 'loaders':1, 'transformers':2, 'reporters':3 # Adicionando permiss\u00f5es apenas usu\u00e1rios do grupo 'transformers' e privil\u00e9gios para um determinado user for schema in folders : cur . execute ( f ''' grant usage on schema \" { schema } \" to group { key } ; grant select on all tables in schema \" { schema } \" to group { key } ; alter default privileges for user { redshift_db_user } in schema \" { schema } \" grant select on tables to group { key } ; ''' ) return True except Exception as e : logging . error ( e ) return False Cria\u00e7\u00e3o das tabelas e copy \u00b6 A fun\u00e7\u00e3o m_aws.csv_to_redshift() \u00e9 um pouco maior, ela cont\u00e9m outras fun\u00e7\u00f5es em seu corpo. Em resumo ela pega o objeto, faz a leitura utilizando o io.BytesIO() , verifica quais s\u00e3o os delimitadores dos objetos .csv , qual o tipo de dado da coluna, qual o tamanho m\u00e1ximo nos registros da coluna e realiza o copy dos dados para o Redshift. python\\engdados\\s3_redshift_load_files.py # 10- Cria\u00e7\u00e3o das tabelas, colunas e copy m_aws . csv_to_redshift ( cur , s3 , bucket_name , list_objects , redshift_details , redshift_db_name ) # 11- fechando conex\u00e3o rd_con . close () code function csv_to_redshift() python\\engdados\\mod_aws.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 def csv_to_redshift ( cur , s3 , bucket_name , list_objects , redshift_details , redshift_db_name ): # Reading the individual files from the AWS S3 buckets and putting them in dataframes redshift_iam_arn , redshift_secrete_name , redshift_region_name = redshift_details for file in list_objects : obj = s3 . Object ( bucket_name , file ) data = obj . get ()[ 'Body' ] . read () # Identificando o delimitador delimiter_csv = csv_identify_delimiter ( data . decode ( 'utf-8' )) # Leitura de csv csv_df = pd . read_csv ( io . BytesIO ( data ), header = 0 , delimiter = delimiter_csv , low_memory = False ) columns_df = csv_df . columns print ( '' , '####### Iniciando tratamento csv ... ##########' , sep = ' \\n ' ) # resevando nomes dos schemas, tabelas e colunas schema = '\"' + file . split ( '/' )[ 0 ] + '\"' table = file . split ( '/' )[ - 1 ] . lower () table = table . replace ( '.csv' , '' ) . replace ( '.CSV' , '' ) print ( '' , 'Tratamento de schema e tabela finalizado.' , sep = ' \\n ' ) print ( '' , \"Pasta para schema: \" , schema , '' , \"Arquivo para tabela: \" , table , sep = ' \\n ' ) # Tratamento das colunas, identifica\u00e7\u00e3o de datatype e length columns , columns_names = csv_column_dtype ( csv_df , columns_df ) # Cria\u00e7\u00e3o das tabelas e colunas cur . execute ( f \"CREATE TABLE IF NOT EXISTS { redshift_db_name } . { schema } . { table }{ columns } ;\" ) print ( '' , \"####### Comando executado: ##########\" , sep = ' \\n ' ) print ( f \"CREATE TABLE IF NOT EXISTS { redshift_db_name } . { schema } . { table }{ columns } ;\" ) # Copy CSV do S3 para o Redshift cur . execute ( f \"\"\" copy { schema } . { table } { columns_names } from 's3:// { bucket_name } / { file } ' iam_role ' { redshift_iam_arn } ' delimiter ' { delimiter_csv } ' region ' { redshift_region_name } ' IGNOREHEADER 1 DATEFORMAT AS 'YYYY-MM-DD HH:MI:SS' removequotes maxerror 3; \"\"\" ) print ( '' , \"Copy do Objeto Finalizado\" , sep = ' \\n ' ) Check Objetivos \u00b6 Os \u00edcones s\u00e3o os finalizados e os s\u00e3o os em abertos. Pegar dados dos arquivos terraform.tfstate Pegar os arquivos csv's armazenados na amazom S3 Pegar dados armazenados no Secrets Manager Criar usu\u00e1rios, grupos e v\u00ednculos Permiss\u00f5es aos grupos e listagem de pastas Permiss\u00f5es e cria\u00e7\u00e3o de schemas Cria\u00e7\u00e3o das tabelas e copy Refatorar os m\u00f3dulos mod_aws e mod_terraform","title":"Usando Python para automatizar"},{"location":"src/engdados/desenvolvimento/python/#python","text":"","title":"Python"},{"location":"src/engdados/desenvolvimento/python/#introducao","text":"Pensando em aumentar a legibilidade, manuten\u00e7\u00e3o e organiza\u00e7\u00e3o do c\u00f3digo. O c\u00f3digo em python est\u00e1 separado em 3 partes: python\\engdados\\s3_redshift_load_files.py : Script main, contendo de forma resumida o c\u00f3digo sem detalhes das fun\u00e7\u00f5es. python\\engdados\\mod_terraform.py : m\u00f3dulo contendo algumas poucas fun\u00e7\u00f5es para auxiliar com os arquivos do terraform. python\\engdados\\mod_aws.py : m\u00f3dulo contendo algumas fun\u00e7\u00f5es para auxiliar nos recursos da AWS.","title":"Introdu\u00e7\u00e3o"},{"location":"src/engdados/desenvolvimento/python/#modulos","text":"Nos trechos abaixo, as fun\u00e7\u00f5es v\u00e3o apresentar os m\u00f3dulos na frente, por exemplo: m_tf.get_path_s3() . m\u00f3dulo abrevia\u00e7\u00e3o mod_terraform m_tf mod_aws m_aws","title":"M\u00f3dulos"},{"location":"src/engdados/desenvolvimento/python/#imports","text":"Imports utilizados no desenvolvimento. Sobre a instala\u00e7\u00e3o dos imports, favor verificar o t\u00f3pico requirements . Caso tenha dificuldades em ler conte\u00fados em ingl\u00eas, sugiro utilizar o navegador google chrome para visualiza\u00e7\u00e3o das documenta\u00e7\u00f5es. Imports descri\u00e7\u00e3o documenta\u00e7\u00e3o boto3 AWS SDK for Python (Boto3) para criar, configurar e gerenciar servi\u00e7os da AWS AWS SDK for Python (Boto3) pandas Ferramenta de an\u00e1lise e manipula\u00e7\u00e3o de dados de c\u00f3digo aberto r\u00e1pida, poderosa, flex\u00edvel e f\u00e1cil de usar, constru\u00edda sobre a linguagem de programa\u00e7\u00e3o Python Guia do usu\u00e1rio io O m\u00f3dulo fornece as principais facilidades do Python para lidar com v\u00e1rios tipos de E/S io Python json A biblioteca json nos permite poder analisar JSON json Python psycopg2 Adaptador de banco de dados PostgreSQL Official Psycopg logging Este m\u00f3dulo define fun\u00e7\u00f5es e classes que implementam um sistema flex\u00edvel de registro de eventos para aplicativos e bibliotecas. logging Python","title":"Imports"},{"location":"src/engdados/desenvolvimento/python/#requirements","text":"Requerimentos utilizados e que necessitam de instala\u00e7\u00e3o. python\\requirements.txt # Requerimentos # Bibliotecas de desenvolvimento pandas # Manipula\u00e7\u00e3o e an\u00e1lise de dados boto3 # Integra\u00e7\u00e3o aos servi\u00e7os AWS psycopg2 # Adaptador do Banco de dados PostgreSQL # Sem pacotes com vers\u00f5es espec\u00edficas Como fa\u00e7o para instalar os requerimentos? Para instala\u00e7\u00e3o de requirements utilize o seguinte comando: pip install -r requirements.txt","title":"Requirements"},{"location":"src/engdados/desenvolvimento/python/#ambiente-virtual","text":"Sobre comandos de cria\u00e7\u00e3o, ativa\u00e7\u00e3o e desativa\u00e7\u00e3o de ambiente virtual. Caso j\u00e1 tenha conhecimento, basta pular para o conex\u00e3o aws . Como usar o ambiente virtual? criar: python - m venv . venv ativar: . \\ . venv \\ Scripts \\ Activate desativar: deactivate","title":"Ambiente virtual"},{"location":"src/engdados/desenvolvimento/python/#conex\u00e3o-aws","text":"Para a conex\u00e3o estarei utilizando o boto3 . Estarei utilizando o profile_name para realizar a sess\u00e3o e posteriomente o cliente. Necessita da CLI AWS s3_redshift_load_files.py # Conex\u00e3o ao Redshift \"\"\"Accessing the S3 buckets using boto3 client\"\"\" session = boto3 . Session ( profile_name = 'ayltonaguiar' ) s3_client = session . client ( 's3' ) s3 = session . resource ( 's3' )","title":"Conex\u00e3o AWS"},{"location":"src/engdados/desenvolvimento/python/#pegando-os-arquivos-tfstate","text":"Fiz a leitura buscando em 2 lugares diferentes. A primeira parte \u00e9 identifica\u00e7\u00e3o v\u00e1lida dos arquivos (verificar se existem), ent\u00e3o foram criadas 2 fun\u00e7\u00f5es com buscas de caminhos espec\u00edficos: m_tf.get_path_s3() e m_tf.get_path_redshift() . Ambas retornam os caminhos (path) dos arquivos. A segunda parte \u00e9 a leitura dos arquivos identificados. Foram criadas outras 2 fun\u00e7\u00f5es com par\u00e2metros espec\u00edficos para cada uma: m_tf.read_s3_tfstate_backend() e m_tf.read_redshift_tfstate() . Ambas retornam informa\u00e7\u00f5es, por exemplo: regi\u00e3o do redshift, regi\u00e3o do bucket backend (remote state), etc. python\\engdados\\s3_redshift_load_files.py # 1- Pega o diret\u00f3rio dos arquivos, valida e guarda as informa\u00e7\u00f5es b\u00e1sicas em novas vari\u00e1veis path_s3 = m_tf . get_path_s3 () path_redshift = m_tf . get_path_redshift () ## 1.1- s3 backend_details = m_tf . read_s3_tfstate_backend ( f \" { path_s3 } \" ) backend_bucket , backend_region , backend_key = backend_details ## 1.2- redshift redshift_details = m_tf . read_redshift_tfstate ( f \" { path_redshift } \" ) redshift_iam_arn , redshift_secrete_name , redshift_region_name = redshift_details code function get_path_s3() python\\engdados\\mod_terraform.py 13 14 15 16 17 18 def get_path_s3 (): # captura do arquivo terraform.state path_s3_tfstate = Path () . absolute () . parent . parent . joinpath ( 'terraform' , '02_aws_s3_and_files' , '.terraform' , 'terraform.tfstate' ) # valida\u00e7\u00e3o do path path_validate ( path_s3_tfstate , get_path_s3 . __name__ ) return path_s3_tfstate code function get_path_redshift() python\\engdados\\mod_terraform.py 20 21 22 23 24 25 def get_path_redshift (): # captura do arquivo terraform.state path_redshift_tfstate = Path () . absolute () . parent . parent . joinpath ( 'terraform' , '03_aws_redshift' , 'terraform.tfstate' ) # valida\u00e7\u00e3o do path path_validate ( path_redshift_tfstate , get_path_redshift . __name__ ) return path_redshift_tfstate code function read_s3_tfstate_backend() python\\engdados\\mod_terraform.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def read_s3_tfstate_backend ( path ): try : # Captura as informa\u00e7\u00f5es pelo backend do tfstate with open ( path ) as file_name : s3_details_backend = json . load ( file_name ) backend_bucket = s3_details_backend [ 'backend' ][ 'config' ] . get ( 'bucket' ) backend_region = s3_details_backend [ 'backend' ][ 'config' ] . get ( 'region' ) backend_key = s3_details_backend [ 'backend' ][ 'config' ] . get ( 'key' ) print ( '' , \"############ S3 Backend ##########\" , s3_details_backend [ 'backend' ][ 'config' ], sep = ' \\n ' ) return [ backend_bucket , backend_region , backend_key ] except Exception as e : logging . error ( e ) return False code function read_redshift_tfstate() python\\engdados\\mod_terraform.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def read_redshift_tfstate ( path ): # Captura dos outputs criados no arquivo terraform.tfstate da pasta aws_redshift try : with open ( path ) as rd_terraform : rd_terraform_json = json . load ( rd_terraform ) redshift_iam_arn = rd_terraform_json [ 'outputs' ] . get ( 'iam_role_arn' ) . get ( 'value' ) redshift_secrete_name = rd_terraform_json [ 'outputs' ] . get ( 'secrete_name' ) . get ( 'value' ) redshift_region_name = rd_terraform_json [ 'outputs' ] . get ( 'region_name' ) . get ( 'value' ) print ( '' , \"####### Outputs redshift.tfstate ##########\" , redshift_iam_arn , redshift_secrete_name , redshift_region_name , sep = ' \\n ' ) return [ redshift_iam_arn , redshift_secrete_name , redshift_region_name ] except Exception as e : logging . error ( e ) return False","title":"Pegando os arquivos terraform.tfstate"},{"location":"src/engdados/desenvolvimento/python/#pegar-arquivos-armazenados-no-bucket-S3","text":"Para identificar objetos no S3, precisamos do nome do bucket e dos arquivos a serem lidos. Para facilitar esse processo, foi criada a fun\u00e7\u00e3o m_aws.get_bucket() . Essa fun\u00e7\u00e3o precisa de informa\u00e7\u00f5es como o nome do bucket compartilhado, a chave do bucket backend e a conex\u00e3o do cliente s3. Ela retornar\u00e1 o nome do bucket que armazena os objetos. A fun\u00e7\u00e3o m_aws.get_csv_s3() verifica e lista os objetos com extens\u00e3o '.csv' ou '.CSV', retornando a lista de objetos. python\\engdados\\s3_redshift_load_files.py # 2- seleciona o objeto do bucket/key fornecido e pega o valor do output 'bucket-name' bucket_name = m_aws . get_bucket ( s3_client , backend_bucket , backend_key ) # 3- Listagem de objetos do bucket fornecido list_objects = m_aws . get_csv_s3 ( s3_client , bucket_name ) code function get_bucket() python\\engdados\\mod_aws.py 8 9 10 11 12 13 14 15 16 17 18 19 20 def get_bucket ( s3_client , backend_bucket , backend_key ): # Identificar bucket remote state try : s3_object = s3_client . get_object ( Bucket = backend_bucket , Key = backend_key ) object_file = s3_object [ \"Body\" ] . read () . decode () parsed_object = json . loads ( object_file ) bucket_name = parsed_object [ 'outputs' ] . get ( 'bucket-name' ) . get ( 'value' ) print ( '' , \"####### Bucket Name Object S3 ##########\" , bucket_name , sep = ' \\n ' ) except Exception as e : logging . error ( e ) return False return bucket_name code function get_csv_s3() python\\engdados\\mod_aws.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def get_csv_s3 ( s3_client , bucket_name ): # Lista os Objetos do bucket setado e armazena na vari\u00e1vel s3_objects try : s3_list_obj = s3_client . list_objects ( Bucket = bucket_name ) s3_objects = [] for obj in s3_list_obj [ 'Contents' ]: if obj [ 'Key' ] . endswith ( 'csv' ) or obj [ 'Key' ] . endswith ( 'CSV' ): s3_objects . append ( obj [ 'Key' ]) print ( '' , \"####### Objects S3 ##########\" , s3_objects [ 0 : 10 ], sep = ' \\n ' ) except Exception as e : logging . error ( e ) return False return s3_objects","title":"Pegar arquivos armazenados no bucket S3"},{"location":"src/engdados/desenvolvimento/python/#pegar-dados-armazenados-no-secrets-manager","text":"Para dar prosseguimento precisamos recuperar alguns dados sens\u00edveis adicionados no secrets manager, ent\u00e3o foi criado a fun\u00e7\u00e3o m_aws.get_secrets_redshift() com objetivo de recuperar essas informa\u00e7\u00f5es e armazen\u00e1-las para o pr\u00f3ximo passo. A conex\u00e3o com banco do redshift \u00e9 feita, nesse projeto, pelo uso do psycopg2. Utilizando as informa\u00e7\u00f5es recuperadas pela fun\u00e7\u00e3o m_aws.get_secrets_redshift() . python\\engdados\\s3_redshift_load_files.py # 4- Acessando e pegando os valores guardados do Redshift no Secrets Manager client_secret = session . client ( 'secretsmanager' , region_name = redshift_region_name ) secrets_manager_details = m_aws . get_secrets_redshift ( client_secret , redshift_secrete_name ) redshift_db_name , redshift_db_user , redshift_db_password , redshift_db_port , redshift_db_host = secrets_manager_details # 5- conex\u00e3o ao banco rd_con = psycopg2 . connect ( host = redshift_db_host , database = redshift_db_name , user = redshift_db_user , password = redshift_db_password , port = redshift_db_port ) rd_con . autocommit = True cur = rd_con . cursor () code function get_secrets_redshift() python\\engdados\\mod_aws.py 40 41 42 43 44 45 46 47 48 49 def get_secrets_redshift ( client_secret , redshift_secrete_name ): try : # Captura dos segredos do redshift na AWS Secrets get_secret_value_response = client_secret . get_secret_value ( SecretId = redshift_secrete_name ) secret_json = json . loads ( get_secret_value_response [ 'SecretString' ]) except Exception as e : logging . error ( e ) return False return secret_json","title":"Pegar dados armazenados no Secrets Manager"},{"location":"src/engdados/desenvolvimento/python/#criar-usuarios-grupos-vinculos-python","text":"A fun\u00e7\u00e3o m_aws.create_users_redshift() tem como objetivo criar os usu\u00e1rios, grupos e realizar vinculos entre as partes. Por \u00faltimo a limpeza do arquivo com dados do secrets manager. python\\engdados\\s3_redshift_load_files.py ## 5.1 Criando os usu\u00e1rios dos grupos loaders, transformers e reporters m_aws . create_users_redshift ( cur , secrets_manager_json ) ## 5.2 Apagando as informa\u00e7\u00f5es do secrets del secrets_manager_json code function create_users_redshift() python\\engdados\\mod_aws.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def create_users_redshift ( cur , secret_json ): try : # Usu\u00e1rios dos grupos - Falta melhorar essa parte bem hardcode e.e Deus me perdoe, mas \u00e9 isso a\u00ed. loaders = [] reporters = [] transformers = [] # Pegando usu\u00e1rios e senhas dos loaders, transformers e reporters for key , value in secret_json . items (): if 'loaders' in key : loaders . append ( value ) elif 'reporters' in key : reporters . append ( value ) elif 'transformers' in key : transformers . append ( value ) # Gerando novas listas a cada 2 items na lista original loaders = [ loaders [ i : i + 2 ] for i in range ( 0 , len ( loaders ), 2 )] reporters = [ reporters [ i : i + 2 ] for i in range ( 0 , len ( reporters ), 2 )] transformers = [ transformers [ i : i + 2 ] for i in range ( 0 , len ( transformers ), 2 )] groups = loaders , reporters , transformers group_names = [ \"loaders\" , \"reporters\" , \"transformers\" ] # Cria\u00e7\u00e3o dos grupos for group in group_names : cur . execute ( f \"CREATE GROUP { group } ;\" ) # Criando Usu\u00e1rios com senhas (0=nome do usu\u00e1rio, 1=senha) for group in groups : for user in group : cur . execute ( f \"create user { user [ 0 ] } with password ' { user [ 1 ] } ';\" ) # Adi\u00e7\u00e3o dos usu\u00e1rios aos grupos for idx , group in enumerate ( groups ): for idx2 , user in enumerate ( group ): cur . execute ( f \"alter group { group_names [ idx ] } add user { user [ 0 ] } ;\" ) except Exception as e : logging . error ( e ) return False","title":"Cria\u00e7\u00e3o de usu\u00e1rios, grupos e vinculos"},{"location":"src/engdados/desenvolvimento/python/#permiss\u00f5es-e-listagem-de-pastas","text":"Lembra que criamos os usu\u00e1rios e grupos de usu\u00e1rios? Ent\u00e3o, estaremos realizando a primeira parte das permiss\u00f5es para os usu\u00e1rios criados, ap\u00f3s essa etapa ser\u00e1 realizado a prepara\u00e7\u00e3o para os nomes das pastas. Para adi\u00e7\u00e3o de permiss\u00f5es, a fun\u00e7\u00e3o m_aws.give_permissions_database() receber\u00e1 a conex\u00e3o, o nome do banco de dados e nome do grupo. A fun\u00e7\u00e3o m_aws.get_folder() pega a primeira parte do diret\u00f3rio da lista de objetos e armazena em SCHEMAS_REDSHIFT . python\\engdados\\s3_redshift_load_files.py # 6- Permiss\u00e3o Create para loaders, select para transformers no banco de dados m_aws . give_permissions_database ( cur , redshift_db_name , groups_redshift ) # 7- pegar o primeiro diret\u00f3rio e adiciona-lo a uma lista de schema. schemas_redshift = [] schemas_redshift = m_aws . get_folder ( list_objects ) code function give_permissions_database() python\\engdados\\mod_aws.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def give_permissions_database ( cur , redshift_db_name , groups ): try : group_exception = [ 'reporters' ] groups_count = group_exception # Executando permiss\u00e3o por grupo n\u00e3o presente em 'groups_count' for group in groups . keys (): if group not in groups_count : cur . execute ( f ''' grant create on database { redshift_db_name } to group { group } ; grant select on all tables in schema information_schema to group { group } ; grant select on all tables in schema pg_catalog to group { group } ; ''' ) groups_count . append ( group ) except Exception as e : logging . error ( e ) return False code function get_folder() python\\engdados\\mod_aws.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def get_folder ( list_objects ): # Cria\u00e7\u00e3o da primeira parte do diret\u00f3rio como schema no banco de dados redshift try : folders = [] for folder in list_objects : folder = folder . split ( '/' , 1 )[ 0 ] if ( folder not in folders ): # Adiciona apenas as pastas diferentes folders . append ( folder . split ( '/' , 1 )[ 0 ]) return folders except Exception as e : logging . error ( e ) return False #print(folders)","title":"Permiss\u00f5es aos grupos e listagem de pastas"},{"location":"src/engdados/desenvolvimento/python/#cria\u00e7\u00e3o-e-permiss\u00f5es-para-schemas","text":"Nessa etapa criaremos os schemas e adicionaremos as permiss\u00f5es para os usu\u00e1rios do grupo 'transformers'. A fun\u00e7\u00e3o m_aws.create_schema_redshift() cria os schemas com base nos diret\u00f3rios armazenados em schemas_redshift . Os Tranformadores precisam de acesso aos schemas criados, pois v\u00e3o trabalhar com todas as tabelas existentes. A fun\u00e7\u00e3o m_aws.give_permission_schemas() visa permitir ao grupo selecionado tenha permiss\u00f5es, al\u00e9m de fazer com que o redshift_db_user ganhe privil\u00e9gios para as novas tabelas criadas. python\\engdados\\s3_redshift_load_files.py # 8- Cria\u00e7\u00e3o dos schemas caso n\u00e3o exista m_aws . create_schema_redshift ( cur , schemas_redshift , redshift_db_user ) # 9- Permiss\u00e3o para o grupo transformers sobre os schemas criados m_aws . give_permission_schemas ( cur , schemas_redshift , redshift_db_user , group_transformers ) code function create_schema_redshift() python\\engdados\\mod_aws.py 165 166 167 168 169 170 171 172 173 174 175 def create_schema_redshift ( cur , folders , redshift_db_user ): # Cria\u00e7\u00e3o da primeira parte do diret\u00f3rio como schema no banco de dados redshift try : for schema in folders : print ( '' , \"####### Comando executado: ##########\" , sep = ' \\n ' ) print ( f 'CREATE SCHEMA IF NOT EXISTS \" { schema } \" AUTHORIZATION { redshift_db_user } ;' ) cur . execute ( f 'CREATE SCHEMA IF NOT EXISTS \" { schema } \" AUTHORIZATION { redshift_db_user } ;' ) # vai criar a pasta entre \"\", exemplo: \"c&a\" except Exception as e : logging . error ( e ) return False return True code function give_permission_schemas() python\\engdados\\mod_aws.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def give_permission_schemas ( cur , folders , redshift_db_user , group ): try : for key , value in group . items (): if value == 2 : # 'loaders':1, 'transformers':2, 'reporters':3 # Adicionando permiss\u00f5es apenas usu\u00e1rios do grupo 'transformers' e privil\u00e9gios para um determinado user for schema in folders : cur . execute ( f ''' grant usage on schema \" { schema } \" to group { key } ; grant select on all tables in schema \" { schema } \" to group { key } ; alter default privileges for user { redshift_db_user } in schema \" { schema } \" grant select on tables to group { key } ; ''' ) return True except Exception as e : logging . error ( e ) return False","title":"Cria\u00e7\u00e3o e Permiss\u00f5es para Schemas"},{"location":"src/engdados/desenvolvimento/python/#cria\u00e7\u00e3o-de-tabelas-e-copys","text":"A fun\u00e7\u00e3o m_aws.csv_to_redshift() \u00e9 um pouco maior, ela cont\u00e9m outras fun\u00e7\u00f5es em seu corpo. Em resumo ela pega o objeto, faz a leitura utilizando o io.BytesIO() , verifica quais s\u00e3o os delimitadores dos objetos .csv , qual o tipo de dado da coluna, qual o tamanho m\u00e1ximo nos registros da coluna e realiza o copy dos dados para o Redshift. python\\engdados\\s3_redshift_load_files.py # 10- Cria\u00e7\u00e3o das tabelas, colunas e copy m_aws . csv_to_redshift ( cur , s3 , bucket_name , list_objects , redshift_details , redshift_db_name ) # 11- fechando conex\u00e3o rd_con . close () code function csv_to_redshift() python\\engdados\\mod_aws.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 def csv_to_redshift ( cur , s3 , bucket_name , list_objects , redshift_details , redshift_db_name ): # Reading the individual files from the AWS S3 buckets and putting them in dataframes redshift_iam_arn , redshift_secrete_name , redshift_region_name = redshift_details for file in list_objects : obj = s3 . Object ( bucket_name , file ) data = obj . get ()[ 'Body' ] . read () # Identificando o delimitador delimiter_csv = csv_identify_delimiter ( data . decode ( 'utf-8' )) # Leitura de csv csv_df = pd . read_csv ( io . BytesIO ( data ), header = 0 , delimiter = delimiter_csv , low_memory = False ) columns_df = csv_df . columns print ( '' , '####### Iniciando tratamento csv ... ##########' , sep = ' \\n ' ) # resevando nomes dos schemas, tabelas e colunas schema = '\"' + file . split ( '/' )[ 0 ] + '\"' table = file . split ( '/' )[ - 1 ] . lower () table = table . replace ( '.csv' , '' ) . replace ( '.CSV' , '' ) print ( '' , 'Tratamento de schema e tabela finalizado.' , sep = ' \\n ' ) print ( '' , \"Pasta para schema: \" , schema , '' , \"Arquivo para tabela: \" , table , sep = ' \\n ' ) # Tratamento das colunas, identifica\u00e7\u00e3o de datatype e length columns , columns_names = csv_column_dtype ( csv_df , columns_df ) # Cria\u00e7\u00e3o das tabelas e colunas cur . execute ( f \"CREATE TABLE IF NOT EXISTS { redshift_db_name } . { schema } . { table }{ columns } ;\" ) print ( '' , \"####### Comando executado: ##########\" , sep = ' \\n ' ) print ( f \"CREATE TABLE IF NOT EXISTS { redshift_db_name } . { schema } . { table }{ columns } ;\" ) # Copy CSV do S3 para o Redshift cur . execute ( f \"\"\" copy { schema } . { table } { columns_names } from 's3:// { bucket_name } / { file } ' iam_role ' { redshift_iam_arn } ' delimiter ' { delimiter_csv } ' region ' { redshift_region_name } ' IGNOREHEADER 1 DATEFORMAT AS 'YYYY-MM-DD HH:MI:SS' removequotes maxerror 3; \"\"\" ) print ( '' , \"Copy do Objeto Finalizado\" , sep = ' \\n ' )","title":"Cria\u00e7\u00e3o das tabelas e copy"},{"location":"src/engdados/desenvolvimento/python/#check-objetivos","text":"Os \u00edcones s\u00e3o os finalizados e os s\u00e3o os em abertos. Pegar dados dos arquivos terraform.tfstate Pegar os arquivos csv's armazenados na amazom S3 Pegar dados armazenados no Secrets Manager Criar usu\u00e1rios, grupos e v\u00ednculos Permiss\u00f5es aos grupos e listagem de pastas Permiss\u00f5es e cria\u00e7\u00e3o de schemas Cria\u00e7\u00e3o das tabelas e copy Refatorar os m\u00f3dulos mod_aws e mod_terraform","title":" Check Objetivos"},{"location":"src/engdados/desenvolvimento/referencias/","text":"Refer\u00eancias \u00b6 Foram utilizadas as seguintes refer\u00eancias para a produ\u00e7\u00e3o do conte\u00fado: Mkdocs \u00b6 Mkdocs Mkdocs-Material Mkdocs-Material-github dbt \u00b6 dbt courses dbt-github hub-getdbt F\u00f3rum da comunidade dbt Amazon \u00b6 Documenta\u00e7\u00e3o do Amazon Simple Storage Service Documenta\u00e7\u00e3o do Amazon Redshift Documenta\u00e7\u00e3o do AWS Secrets Manager Python \u00b6 Documenta\u00e7\u00e3o do Boto3 Site Python: V\u00e1rias documenta\u00e7\u00f5es Pypi, Para procurar e instalar pacotes Pandas, Guia do usu\u00e1rio Terraform \u00b6 Documenta\u00e7\u00e3o Terraform Registro Terraform: M\u00f3dulos e Provedores","title":"Refer\u00eancias"},{"location":"src/engdados/desenvolvimento/referencias/#referencias","text":"Foram utilizadas as seguintes refer\u00eancias para a produ\u00e7\u00e3o do conte\u00fado:","title":"Refer\u00eancias"},{"location":"src/engdados/desenvolvimento/referencias/#mkdocs","text":"Mkdocs Mkdocs-Material Mkdocs-Material-github","title":"Mkdocs"},{"location":"src/engdados/desenvolvimento/referencias/#dbt","text":"dbt courses dbt-github hub-getdbt F\u00f3rum da comunidade dbt","title":"dbt"},{"location":"src/engdados/desenvolvimento/referencias/#amazon","text":"Documenta\u00e7\u00e3o do Amazon Simple Storage Service Documenta\u00e7\u00e3o do Amazon Redshift Documenta\u00e7\u00e3o do AWS Secrets Manager","title":"Amazon"},{"location":"src/engdados/desenvolvimento/referencias/#python","text":"Documenta\u00e7\u00e3o do Boto3 Site Python: V\u00e1rias documenta\u00e7\u00f5es Pypi, Para procurar e instalar pacotes Pandas, Guia do usu\u00e1rio","title":"Python"},{"location":"src/engdados/desenvolvimento/referencias/#terraform","text":"Documenta\u00e7\u00e3o Terraform Registro Terraform: M\u00f3dulos e Provedores","title":"Terraform"}]}